{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aadaa10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: IMPORT LIBRARIES\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils import get_column_letter\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"[OK] Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e7b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Temuan (Known Fraud): 52 rows\n",
      "Data History: 152974 rows\n",
      "\n",
      "Known Fraud (label=1): 61 pelanggan\n",
      "Unknown (label=0): 152913 pelanggan\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: LOAD DATA\n",
    "# =============================================================================\n",
    "df_temuan = pd.read_excel('../data/Temuan 2425.xlsx')\n",
    "df_history = pd.read_excel('../data/Histori Pemakaian Pelanggan.xlsx')\n",
    "\n",
    "print(f\"Data Temuan (Known Fraud): {len(df_temuan)} rows\")\n",
    "print(f\"Data History: {len(df_history)} rows\")\n",
    "\n",
    "# Kolom di Temuan: IDE (bukan IDPEL)\n",
    "# Kolom di History: IDE, UE, lalu kolom tanggal\n",
    "ide_temuan = set(df_temuan['IDE'].astype(str).str.strip())\n",
    "df_history['IDE'] = df_history['IDE'].astype(str).str.strip()\n",
    "df_history['UE'] = df_history['UE'].astype(str).str.strip()\n",
    "df_history['label'] = df_history['IDE'].apply(lambda x: 1 if x in ide_temuan else 0)\n",
    "\n",
    "print(f\"\\nKnown Fraud (label=1): {df_history['label'].sum()} pelanggan\")\n",
    "print(f\"Unknown (label=0): {(df_history['label']==0).sum()} pelanggan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455a0c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period: 2021-03-01 00:00:00 to 2026-01-01 00:00:00 (59 months)\n",
      "\n",
      "==================================================\n",
      "ELIGIBILITY SUMMARY\n",
      "==================================================\n",
      "  ELIGIBLE: 132,738 (86.8%)\n",
      "  INACTIVE_STOPPED: 14,320 (9.4%)\n",
      "  INSUFFICIENT_HISTORY: 5,916 (3.9%)\n",
      "\n",
      "ELIGIBLE for prediction: 132,738\n",
      "NOT ELIGIBLE: 20,236\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: DATA PREPROCESSING & ELIGIBILITY FILTERING\n",
    "# =============================================================================\n",
    "non_date_cols = ['IDE', 'UE', 'label']\n",
    "date_columns = [col for col in df_history.columns if col not in non_date_cols]\n",
    "\n",
    "print(f\"Period: {date_columns[0]} to {date_columns[-1]} ({len(date_columns)} months)\")\n",
    "\n",
    "MIN_ACTIVE_MONTHS = 12\n",
    "\n",
    "def find_active_period(row):\n",
    "    not_na_mask = ~pd.isna(row)\n",
    "    if not_na_mask.sum() == 0:\n",
    "        return None, None\n",
    "    indices = np.where(not_na_mask)[0]\n",
    "    return indices[0], indices[-1]\n",
    "\n",
    "active_periods = []\n",
    "for idx, row in df_history[date_columns].iterrows():\n",
    "    start_idx, end_idx = find_active_period(row.values)\n",
    "    active_length = (end_idx - start_idx + 1) if start_idx is not None else 0\n",
    "    active_periods.append({'start_idx': start_idx, 'end_idx': end_idx, 'active_months': active_length})\n",
    "\n",
    "df_history['active_start_idx'] = [p['start_idx'] for p in active_periods]\n",
    "df_history['active_end_idx'] = [p['end_idx'] for p in active_periods]\n",
    "df_history['active_months_count'] = [p['active_months'] for p in active_periods]\n",
    "\n",
    "last_month_col = date_columns[-1]\n",
    "df_history['is_still_active'] = ~pd.isna(df_history[last_month_col])\n",
    "\n",
    "def determine_eligibility(row):\n",
    "    if not row['is_still_active']:\n",
    "        return 'INACTIVE_STOPPED'\n",
    "    elif row['active_months_count'] < MIN_ACTIVE_MONTHS:\n",
    "        return 'INSUFFICIENT_HISTORY'\n",
    "    else:\n",
    "        return 'ELIGIBLE'\n",
    "\n",
    "df_history['prediction_eligibility'] = df_history.apply(determine_eligibility, axis=1)\n",
    "\n",
    "for col in date_columns:\n",
    "    df_history[col] = pd.to_numeric(df_history[col], errors='coerce')\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"ELIGIBILITY SUMMARY\")\n",
    "print('='*50)\n",
    "for status, count in df_history['prediction_eligibility'].value_counts().items():\n",
    "    pct = count / len(df_history) * 100\n",
    "    print(f\"  {status}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "df_eligible = df_history[df_history['prediction_eligibility'] == 'ELIGIBLE'].copy()\n",
    "df_not_eligible = df_history[df_history['prediction_eligibility'] != 'ELIGIBLE'].copy()\n",
    "\n",
    "print(f\"\\nELIGIBLE for prediction: {len(df_eligible):,}\")\n",
    "print(f\"NOT ELIGIBLE: {len(df_not_eligible):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712e4ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 13 Basic Features...\n",
      "[OK] 13 Basic Features extracted\n",
      "Features: ['usage_mean', 'usage_std', 'usage_min', 'usage_max', 'usage_range', 'coefficient_variation', 'zero_usage_count', 'max_drop', 'trend_slope', 'recent_disconnect', 'inactive_months', 'ue_encoded', 'ue_fraud_risk']\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: FEATURE ENGINEERING - 13 BASIC FEATURES\n",
    "# =============================================================================\n",
    "def extract_active_data(row, date_cols):\n",
    "    start_idx = int(row['active_start_idx']) if not pd.isna(row['active_start_idx']) else 0\n",
    "    end_idx = int(row['active_end_idx']) if not pd.isna(row['active_end_idx']) else 0\n",
    "    data = row[date_cols].values[start_idx:end_idx+1]\n",
    "    return np.array([0 if pd.isna(x) else x for x in data], dtype=float)\n",
    "\n",
    "print(\"Extracting 13 Basic Features...\")\n",
    "df_features = df_eligible.copy()\n",
    "\n",
    "features_data = []\n",
    "for idx, row in df_features.iterrows():\n",
    "    data = extract_active_data(row, date_columns)\n",
    "    if len(data) == 0:\n",
    "        data = np.array([0])\n",
    "    \n",
    "    mean_val = np.mean(data)\n",
    "    std_val = np.std(data)\n",
    "    \n",
    "    features_data.append({\n",
    "        'usage_mean': mean_val,\n",
    "        'usage_std': std_val,\n",
    "        'usage_min': np.min(data),\n",
    "        'usage_max': np.max(data),\n",
    "        'usage_range': np.max(data) - np.min(data),\n",
    "        'coefficient_variation': std_val / mean_val if mean_val > 0 else 0,\n",
    "        'zero_usage_count': (data == 0).sum(),\n",
    "        'max_drop': max(0, -np.min(np.diff(data))) if len(data) > 1 else 0,\n",
    "        'trend_slope': np.polyfit(range(len(data)), data, 1)[0] if len(data) > 1 and np.std(data) > 0 else 0,\n",
    "        'recent_disconnect': (data[-3:] < 10).sum() if len(data) >= 3 else 0,\n",
    "        'inactive_months': (data < 5).sum()\n",
    "    })\n",
    "\n",
    "for feat in features_data[0].keys():\n",
    "    df_features[feat] = [f[feat] for f in features_data]\n",
    "\n",
    "le_ue = LabelEncoder()\n",
    "df_features['ue_encoded'] = le_ue.fit_transform(df_features['UE'].fillna('UNKNOWN'))\n",
    "\n",
    "ue_fraud_rate = df_features.groupby('UE')['label'].mean()\n",
    "df_features['ue_fraud_risk'] = df_features['UE'].map(ue_fraud_rate).fillna(0)\n",
    "\n",
    "basic_features = ['usage_mean', 'usage_std', 'usage_min', 'usage_max', 'usage_range',\n",
    "                  'coefficient_variation', 'zero_usage_count', 'max_drop', 'trend_slope',\n",
    "                  'recent_disconnect', 'inactive_months', 'ue_encoded', 'ue_fraud_risk']\n",
    "\n",
    "print(f\"[OK] 13 Basic Features extracted\")\n",
    "print(f\"Features: {basic_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0217b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 14 Temporal Features (comparing first year vs last year)...\n",
      "[OK] 14 Temporal Features extracted\n",
      "Features: ['first_year_mean', 'last_year_mean', 'year_change_abs', 'year_change_pct', 'first_year_zero', 'last_year_zero', 'zero_increase', 'stability_first', 'stability_last', 'stability_change', 'max_window_drop', 'volatility_change', 'trend_break_score', 'recent_anomaly_score']\n",
      "\n",
      "Total Features: 27\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: FEATURE ENGINEERING - 14 TEMPORAL FEATURES\n",
    "# =============================================================================\n",
    "print(\"Extracting 14 Temporal Features (comparing first year vs last year)...\")\n",
    "\n",
    "temporal_features_list = []\n",
    "for idx, row in df_features.iterrows():\n",
    "    data = extract_active_data(row, date_columns)\n",
    "    \n",
    "    if len(data) < 12:\n",
    "        temporal_features_list.append({k: 0 for k in ['first_year_mean', 'last_year_mean', 'year_change_abs', \n",
    "            'year_change_pct', 'first_year_zero', 'last_year_zero', 'zero_increase', 'stability_first',\n",
    "            'stability_last', 'stability_change', 'max_window_drop', 'volatility_change', \n",
    "            'trend_break_score', 'recent_anomaly_score']})\n",
    "        continue\n",
    "    \n",
    "    first_year = data[:12]\n",
    "    last_year = data[-12:]\n",
    "    \n",
    "    first_mean = np.mean(first_year)\n",
    "    last_mean = np.mean(last_year)\n",
    "    \n",
    "    year_change_pct = ((last_mean - first_mean) / first_mean * 100) if first_mean > 0 else 0\n",
    "    \n",
    "    first_zero = (first_year == 0).sum()\n",
    "    last_zero = (last_year == 0).sum()\n",
    "    \n",
    "    first_std = np.std(first_year)\n",
    "    last_std = np.std(last_year)\n",
    "    \n",
    "    # Max window drop (6-month windows)\n",
    "    window_size = 6\n",
    "    max_drop = 0\n",
    "    if len(data) >= window_size * 2:\n",
    "        for i in range(0, len(data) - window_size * 2 + 1, window_size):\n",
    "            w1 = np.mean(data[i:i+window_size])\n",
    "            w2 = np.mean(data[i+window_size:i+window_size*2])\n",
    "            drop = w1 - w2\n",
    "            if drop > max_drop:\n",
    "                max_drop = drop\n",
    "    \n",
    "    # Trend break\n",
    "    mid = len(data) // 2\n",
    "    slope_first = np.polyfit(range(mid), data[:mid], 1)[0] if mid > 1 else 0\n",
    "    slope_last = np.polyfit(range(len(data) - mid), data[mid:], 1)[0] if len(data) - mid > 1 else 0\n",
    "    \n",
    "    # Recent anomaly\n",
    "    overall_mean = np.mean(data)\n",
    "    overall_std = np.std(data)\n",
    "    recent_mean = np.mean(data[-6:])\n",
    "    anomaly_score = (recent_mean - overall_mean) / overall_std if overall_std > 0 else 0\n",
    "    \n",
    "    temporal_features_list.append({\n",
    "        'first_year_mean': first_mean,\n",
    "        'last_year_mean': last_mean,\n",
    "        'year_change_abs': last_mean - first_mean,\n",
    "        'year_change_pct': year_change_pct,\n",
    "        'first_year_zero': first_zero,\n",
    "        'last_year_zero': last_zero,\n",
    "        'zero_increase': last_zero - first_zero,\n",
    "        'stability_first': first_std,\n",
    "        'stability_last': last_std,\n",
    "        'stability_change': last_std - first_std,\n",
    "        'max_window_drop': max_drop,\n",
    "        'volatility_change': abs(last_std - first_std),\n",
    "        'trend_break_score': abs(slope_first - slope_last),\n",
    "        'recent_anomaly_score': anomaly_score\n",
    "    })\n",
    "\n",
    "temporal_cols = list(temporal_features_list[0].keys())\n",
    "for feat in temporal_cols:\n",
    "    df_features[feat] = [f[feat] for f in temporal_features_list]\n",
    "\n",
    "print(f\"[OK] 14 Temporal Features extracted\")\n",
    "print(f\"Features: {temporal_cols}\")\n",
    "\n",
    "all_feature_cols = basic_features + temporal_cols\n",
    "print(f\"\\nTotal Features: {len(all_feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e035d892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Isolation Forest for anomaly detection...\n",
      "[OK] Anomalies detected: 1,328 (1.00%)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: ISOLATION FOREST (Anomaly Detection)\n",
    "# =============================================================================\n",
    "print(\"Training Isolation Forest for anomaly detection...\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_all = df_features[all_feature_cols].values\n",
    "X_all_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.01, random_state=42, n_jobs=-1)\n",
    "iso_predictions = iso_forest.fit_predict(X_all_scaled)\n",
    "\n",
    "df_features['iso_anomaly'] = (iso_predictions == -1).astype(int)\n",
    "\n",
    "n_anomaly = df_features['iso_anomaly'].sum()\n",
    "print(f\"[OK] Anomalies detected: {n_anomaly:,} ({n_anomaly/len(df_features)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4df58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pseudo-labels...\n",
      "Known Fraud: 41\n",
      "Pseudo Normal: 5000\n",
      "Training set: 5041\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: PSEUDO-LABELING\n",
    "# =============================================================================\n",
    "print(\"Creating pseudo-labels...\")\n",
    "\n",
    "df_known_fraud = df_features[df_features['label'] == 1].copy()\n",
    "df_unknown = df_features[df_features['label'] == 0].copy()\n",
    "\n",
    "likely_normal_mask = (\n",
    "    (df_unknown['iso_anomaly'] == 0) &\n",
    "    (df_unknown['zero_usage_count'] <= 1) &\n",
    "    (df_unknown['coefficient_variation'] < 0.5) &\n",
    "    (df_unknown['year_change_pct'] > -30)\n",
    ")\n",
    "\n",
    "df_likely_normal = df_unknown[likely_normal_mask]\n",
    "n_sample = min(5000, len(df_likely_normal))\n",
    "df_pseudo_normal = df_likely_normal.sample(n=n_sample, random_state=42)\n",
    "\n",
    "df_train = pd.concat([df_known_fraud, df_pseudo_normal])\n",
    "\n",
    "print(f\"Known Fraud: {len(df_known_fraud)}\")\n",
    "print(f\"Pseudo Normal: {len(df_pseudo_normal)}\")\n",
    "print(f\"Training set: {len(df_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2411f975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing with SMOTE...\n",
      "Before SMOTE: Fraud=41, Normal=5000\n",
      "After SMOTE: Fraud=5000, Normal=5000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: SMOTE BALANCING\n",
    "# =============================================================================\n",
    "print(\"Balancing with SMOTE...\")\n",
    "\n",
    "X_train = df_train[all_feature_cols].values\n",
    "y_train = df_train['label'].values\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "k_neighbors = min(5, len(df_known_fraud) - 1)\n",
    "smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Before SMOTE: Fraud={y_train.sum()}, Normal={(y_train==0).sum()}\")\n",
    "print(f\"After SMOTE: Fraud={(y_train_balanced==1).sum()}, Normal={(y_train_balanced==0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4160e85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest...\n",
      "Training XGBoost...\n",
      "[OK] Models trained successfully\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: TRAIN MODELS (Random Forest + XGBoost)\n",
    "# =============================================================================\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=300, max_depth=15, min_samples_split=5,\n",
    "                                   class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = XGBClassifier(n_estimators=200, max_depth=10, learning_rate=0.1,\n",
    "                          scale_pos_weight=10, random_state=42, n_jobs=-1,\n",
    "                          use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "print(\"[OK] Models trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dcbabdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on all eligible customers...\n",
      "\n",
      "Predicted Fraud: 27,756 (20.91%)\n",
      "\n",
      "Priority Distribution:\n",
      "  CRITICAL: 23,151\n",
      "  HIGH: 4,605\n",
      "  MEDIUM: 2,190\n",
      "  LOW: 102,792\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: PREDICTION & ENSEMBLE\n",
    "# =============================================================================\n",
    "print(\"Making predictions on all eligible customers...\")\n",
    "\n",
    "rf_proba = rf_model.predict_proba(X_all_scaled)[:, 1]\n",
    "xgb_proba = xgb_model.predict_proba(X_all_scaled)[:, 1]\n",
    "ensemble_proba = (rf_proba + xgb_proba) / 2\n",
    "\n",
    "df_features['rf_fraud_prob'] = rf_proba\n",
    "df_features['xgb_fraud_prob'] = xgb_proba\n",
    "df_features['ensemble_fraud_prob'] = ensemble_proba\n",
    "df_features['ensemble_pred'] = (ensemble_proba >= 0.5).astype(int)\n",
    "\n",
    "def get_priority(prob):\n",
    "    if prob >= 0.7: return 'CRITICAL'\n",
    "    elif prob >= 0.5: return 'HIGH'\n",
    "    elif prob >= 0.3: return 'MEDIUM'\n",
    "    else: return 'LOW'\n",
    "\n",
    "df_features['priority'] = df_features['ensemble_fraud_prob'].apply(get_priority)\n",
    "\n",
    "pred_fraud = (df_features['ensemble_pred'] == 1).sum()\n",
    "print(f\"\\nPredicted Fraud: {pred_fraud:,} ({pred_fraud/len(df_features)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nPriority Distribution:\")\n",
    "for priority in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:\n",
    "    count = (df_features['priority'] == priority).sum()\n",
    "    print(f\"  {priority}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1959e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL EVALUATION ON KNOWN FRAUD\n",
      "============================================================\n",
      "\n",
      "Known Fraud: 41\n",
      "Detected: 41\n",
      "Recall: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: EVALUATE ON KNOWN FRAUD\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION ON KNOWN FRAUD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "known_fraud = df_features[df_features['label'] == 1]\n",
    "detected = (known_fraud['ensemble_pred'] == 1).sum()\n",
    "total = len(known_fraud)\n",
    "recall = detected / total * 100\n",
    "\n",
    "print(f\"\\nKnown Fraud: {total}\")\n",
    "print(f\"Detected: {detected}\")\n",
    "print(f\"Recall: {recall:.1f}%\")\n",
    "\n",
    "if total > detected:\n",
    "    missed = known_fraud[known_fraud['ensemble_pred'] == 0]\n",
    "    print(f\"\\nMissed ({total - detected}):\")\n",
    "    for _, row in missed.iterrows():\n",
    "        print(f\"  IDE: {row['IDE']}, Prob: {row['ensemble_fraud_prob']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e969f12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE IMPORTANCE (Top 15)\n",
      "============================================================\n",
      "\n",
      "Feature                             RF          XGB      Average\n",
      "-----------------------------------------------------------------\n",
      "ue_fraud_risk                   0.2388       0.4457       0.3422\n",
      "ue_encoded                      0.1488       0.0451       0.0970\n",
      "year_change_pct                 0.0846       0.0515       0.0680\n",
      "coefficient_variation           0.0652       0.0247       0.0449\n",
      "usage_min                       0.0443       0.0389       0.0416\n",
      "max_window_drop                 0.0317       0.0461       0.0389\n",
      "stability_last                  0.0261       0.0438       0.0350\n",
      "usage_std                       0.0206       0.0492       0.0349\n",
      "stability_first                 0.0323       0.0347       0.0335\n",
      "recent_anomaly_score            0.0392       0.0153       0.0272\n",
      "year_change_abs                 0.0363       0.0155       0.0259\n",
      "usage_mean                      0.0216       0.0221       0.0219\n",
      "usage_range                     0.0222       0.0199       0.0210\n",
      "volatility_change               0.0191       0.0219       0.0205\n",
      "first_year_mean                 0.0210       0.0175       0.0192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>RF_Importance</th>\n",
       "      <th>XGB_Importance</th>\n",
       "      <th>Avg_Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ue_fraud_risk</td>\n",
       "      <td>0.238791</td>\n",
       "      <td>0.445686</td>\n",
       "      <td>0.342238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ue_encoded</td>\n",
       "      <td>0.148814</td>\n",
       "      <td>0.045102</td>\n",
       "      <td>0.096958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>year_change_pct</td>\n",
       "      <td>0.084552</td>\n",
       "      <td>0.051512</td>\n",
       "      <td>0.068032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coefficient_variation</td>\n",
       "      <td>0.065218</td>\n",
       "      <td>0.024682</td>\n",
       "      <td>0.044950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>usage_min</td>\n",
       "      <td>0.044350</td>\n",
       "      <td>0.038932</td>\n",
       "      <td>0.041641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>max_window_drop</td>\n",
       "      <td>0.031717</td>\n",
       "      <td>0.046066</td>\n",
       "      <td>0.038891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>stability_last</td>\n",
       "      <td>0.026136</td>\n",
       "      <td>0.043827</td>\n",
       "      <td>0.034982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usage_std</td>\n",
       "      <td>0.020590</td>\n",
       "      <td>0.049187</td>\n",
       "      <td>0.034888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>stability_first</td>\n",
       "      <td>0.032292</td>\n",
       "      <td>0.034684</td>\n",
       "      <td>0.033488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>recent_anomaly_score</td>\n",
       "      <td>0.039151</td>\n",
       "      <td>0.015281</td>\n",
       "      <td>0.027216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>year_change_abs</td>\n",
       "      <td>0.036333</td>\n",
       "      <td>0.015529</td>\n",
       "      <td>0.025931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>usage_mean</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>0.022118</td>\n",
       "      <td>0.021871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>usage_range</td>\n",
       "      <td>0.022207</td>\n",
       "      <td>0.019856</td>\n",
       "      <td>0.021032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>volatility_change</td>\n",
       "      <td>0.019115</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>0.020502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>first_year_mean</td>\n",
       "      <td>0.020988</td>\n",
       "      <td>0.017457</td>\n",
       "      <td>0.019222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Feature  RF_Importance  XGB_Importance  Avg_Importance\n",
       "12          ue_fraud_risk       0.238791        0.445686        0.342238\n",
       "11             ue_encoded       0.148814        0.045102        0.096958\n",
       "16        year_change_pct       0.084552        0.051512        0.068032\n",
       "5   coefficient_variation       0.065218        0.024682        0.044950\n",
       "2               usage_min       0.044350        0.038932        0.041641\n",
       "23        max_window_drop       0.031717        0.046066        0.038891\n",
       "21         stability_last       0.026136        0.043827        0.034982\n",
       "1               usage_std       0.020590        0.049187        0.034888\n",
       "20        stability_first       0.032292        0.034684        0.033488\n",
       "26   recent_anomaly_score       0.039151        0.015281        0.027216\n",
       "15        year_change_abs       0.036333        0.015529        0.025931\n",
       "0              usage_mean       0.021623        0.022118        0.021871\n",
       "4             usage_range       0.022207        0.019856        0.021032\n",
       "24      volatility_change       0.019115        0.021888        0.020502\n",
       "13        first_year_mean       0.020988        0.017457        0.019222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 12: FEATURE IMPORTANCE\n",
    "# =============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE (Top 15)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_importance = rf_model.feature_importances_\n",
    "xgb_importance = xgb_model.feature_importances_\n",
    "avg_importance = (rf_importance + xgb_importance) / 2\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_cols,\n",
    "    'RF_Importance': rf_importance,\n",
    "    'XGB_Importance': xgb_importance,\n",
    "    'Avg_Importance': avg_importance\n",
    "}).sort_values('Avg_Importance', ascending=False)\n",
    "\n",
    "print(\"\\n{:<25} {:>12} {:>12} {:>12}\".format('Feature', 'RF', 'XGB', 'Average'))\n",
    "print(\"-\" * 65)\n",
    "for i, (_, row) in enumerate(importance_df.head(15).iterrows()):\n",
    "    print(f\"{row['Feature']:<25} {row['RF_Importance']:>12.4f} {row['XGB_Importance']:>12.4f} {row['Avg_Importance']:>12.4f}\")\n",
    "\n",
    "importance_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b333e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting theft start months...\n",
      "\n",
      "[OK] Detected 19,166 customers with suspicious months\n",
      "\n",
      "Severity Distribution:\n",
      "  CRITICAL: 10,317\n",
      "  HIGH: 7,267\n",
      "  MEDIUM: 1,582\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 13: THEFT MONTH DETECTION\n",
    "# =============================================================================\n",
    "print(\"Detecting theft start months...\")\n",
    "\n",
    "def detect_theft_months(row, date_cols, prob):\n",
    "    \"\"\"Detect which months show suspicious patterns (likely theft start)\"\"\"\n",
    "    if prob < 0.3:\n",
    "        return []\n",
    "    \n",
    "    data = extract_active_data(row, date_cols)\n",
    "    if len(data) < 6:\n",
    "        return []\n",
    "    \n",
    "    start_idx = int(row['active_start_idx'])\n",
    "    suspicious = []\n",
    "    \n",
    "    # Calculate baseline from first 6 months\n",
    "    baseline = np.mean(data[:6]) if len(data) >= 6 else np.mean(data)\n",
    "    if baseline <= 0:\n",
    "        baseline = np.mean(data[data > 0]) if (data > 0).any() else 1\n",
    "    \n",
    "    # Check each month for anomalies\n",
    "    for i in range(6, len(data)):\n",
    "        month_col = date_cols[start_idx + i]\n",
    "        value = data[i]\n",
    "        prev_6_mean = np.mean(data[max(0, i-6):i])\n",
    "        \n",
    "        # Criteria for suspicious month\n",
    "        is_zero = value == 0 and baseline > 50\n",
    "        is_drop_70 = value < baseline * 0.3 and baseline > 30\n",
    "        is_sudden_drop = prev_6_mean > 0 and value < prev_6_mean * 0.3\n",
    "        \n",
    "        if is_zero or is_drop_70 or is_sudden_drop:\n",
    "            # Determine severity\n",
    "            if is_zero:\n",
    "                severity = 'CRITICAL'\n",
    "            elif value < baseline * 0.2:\n",
    "                severity = 'CRITICAL'\n",
    "            elif value < baseline * 0.3:\n",
    "                severity = 'HIGH'\n",
    "            else:\n",
    "                severity = 'MEDIUM'\n",
    "            \n",
    "            suspicious.append({\n",
    "                'month': month_col,\n",
    "                'value': value,\n",
    "                'baseline': baseline,\n",
    "                'drop_pct': (1 - value/baseline) * 100 if baseline > 0 else 0,\n",
    "                'severity': severity\n",
    "            })\n",
    "    \n",
    "    return suspicious\n",
    "\n",
    "theft_patterns = []\n",
    "for idx, row in df_features.iterrows():\n",
    "    prob = row['ensemble_fraud_prob']\n",
    "    patterns = detect_theft_months(row, date_columns, prob)\n",
    "    \n",
    "    if patterns:\n",
    "        first_suspicious = patterns[0]\n",
    "        theft_patterns.append({\n",
    "            'IDE': row['IDE'],\n",
    "            'UE': row['UE'],\n",
    "            'fraud_prob': prob,\n",
    "            'priority': row['priority'],\n",
    "            'first_theft_month': first_suspicious['month'],\n",
    "            'first_theft_value': first_suspicious['value'],\n",
    "            'baseline_usage': first_suspicious['baseline'],\n",
    "            'drop_percentage': first_suspicious['drop_pct'],\n",
    "            'severity': first_suspicious['severity'],\n",
    "            'suspicious_months_count': len(patterns),\n",
    "            'all_suspicious': patterns\n",
    "        })\n",
    "\n",
    "df_theft = pd.DataFrame(theft_patterns)\n",
    "print(f\"\\n[OK] Detected {len(df_theft):,} customers with suspicious months\")\n",
    "\n",
    "if len(df_theft) > 0:\n",
    "    print(f\"\\nSeverity Distribution:\")\n",
    "    for sev in ['CRITICAL', 'HIGH', 'MEDIUM']:\n",
    "        count = (df_theft['severity'] == sev).sum()\n",
    "        print(f\"  {sev}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f864159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating theft months matrix...\n",
      "[OK] Matrix created: 31089 customers x 59 months\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 14: CREATE THEFT MONTHS MATRIX (for Excel coloring)\n",
    "# =============================================================================\n",
    "print(\"Creating theft months matrix...\")\n",
    "\n",
    "# Get high priority suspects\n",
    "high_priority_ides = df_features[df_features['priority'].isin(['CRITICAL', 'HIGH'])]['IDE'].tolist()\n",
    "\n",
    "# Create matrix with all months\n",
    "matrix_records = []\n",
    "for _, row in df_features[df_features['IDE'].isin(high_priority_ides)].iterrows():\n",
    "    record = {\n",
    "        'IDE': row['IDE'],\n",
    "        'NAMA': row['UE'],\n",
    "        'FRAUD_PROB': round(row['ensemble_fraud_prob'], 3),\n",
    "        'PRIORITY': row['priority'],\n",
    "        'KNOWN_FRAUD': 'YES' if row['label'] == 1 else 'NO'\n",
    "    }\n",
    "    \n",
    "    # Get theft months for this customer\n",
    "    theft_info = df_theft[df_theft['IDE'] == row['IDE']]\n",
    "    suspicious_months = {}\n",
    "    if len(theft_info) > 0:\n",
    "        for pattern in theft_info.iloc[0]['all_suspicious']:\n",
    "            suspicious_months[pattern['month']] = pattern['severity']\n",
    "    \n",
    "    # Add each month column\n",
    "    for col in date_columns:\n",
    "        value = row[col] if not pd.isna(row[col]) else 'N/A'\n",
    "        if col in suspicious_months:\n",
    "            record[col] = f\"{value}|{suspicious_months[col]}\"\n",
    "        else:\n",
    "            record[col] = value\n",
    "    \n",
    "    matrix_records.append(record)\n",
    "\n",
    "df_matrix = pd.DataFrame(matrix_records)\n",
    "print(f\"[OK] Matrix created: {len(df_matrix)} customers x {len(date_columns)} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50c513ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "EXPORTING TO EXCEL WITH COLOR CODING\n",
      "======================================================================\n",
      "Writing data with color coding...\n",
      "\n",
      "[OK] File saved: ../results/electricity_theft_detection_results.xlsx\n",
      "\n",
      "Sheets:\n",
      "  1. Theft_Detection_Matrix - Main results with color-coded theft months\n",
      "  2. Legend - Color meaning explanation\n",
      "  3. Summary - Key metrics\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 15: EXPORT TO EXCEL WITH COLOR CODING\n",
    "# =============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"EXPORTING TO EXCEL WITH COLOR CODING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "output_file = '../results/electricity_theft_detection_results.xlsx'\n",
    "\n",
    "# Create workbook\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "ws.title = 'Theft_Detection_Matrix'\n",
    "\n",
    "# Define styles\n",
    "header_fill = PatternFill(start_color='1F4E79', end_color='1F4E79', fill_type='solid')\n",
    "header_font = Font(bold=True, color='FFFFFF', size=11)\n",
    "\n",
    "critical_fill = PatternFill(start_color='FF0000', end_color='FF0000', fill_type='solid')  # Red\n",
    "high_fill = PatternFill(start_color='FF6600', end_color='FF6600', fill_type='solid')      # Orange\n",
    "medium_fill = PatternFill(start_color='FFCC00', end_color='FFCC00', fill_type='solid')    # Yellow\n",
    "normal_fill = PatternFill(start_color='92D050', end_color='92D050', fill_type='solid')    # Green\n",
    "na_fill = PatternFill(start_color='D9D9D9', end_color='D9D9D9', fill_type='solid')        # Gray\n",
    "\n",
    "critical_font = Font(bold=True, color='FFFFFF')\n",
    "high_font = Font(bold=True, color='000000')\n",
    "medium_font = Font(bold=True, color='000000')\n",
    "\n",
    "thin_border = Border(\n",
    "    left=Side(style='thin'),\n",
    "    right=Side(style='thin'),\n",
    "    top=Side(style='thin'),\n",
    "    bottom=Side(style='thin')\n",
    ")\n",
    "\n",
    "# Write header\n",
    "columns = ['IDE', 'NAMA', 'FRAUD_PROB', 'PRIORITY', 'KNOWN_FRAUD'] + date_columns\n",
    "for col_idx, col_name in enumerate(columns, 1):\n",
    "    cell = ws.cell(row=1, column=col_idx, value=col_name)\n",
    "    cell.fill = header_fill\n",
    "    cell.font = header_font\n",
    "    cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    cell.border = thin_border\n",
    "\n",
    "# Write data with color coding\n",
    "print(\"Writing data with color coding...\")\n",
    "for row_idx, record in enumerate(matrix_records, 2):\n",
    "    for col_idx, col_name in enumerate(columns, 1):\n",
    "        value = record.get(col_name, '')\n",
    "        cell = ws.cell(row=row_idx, column=col_idx)\n",
    "        cell.border = thin_border\n",
    "        cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "        \n",
    "        # Check if it's a month column with severity\n",
    "        if col_name in date_columns and isinstance(value, str) and '|' in str(value):\n",
    "            parts = value.split('|')\n",
    "            cell.value = float(parts[0]) if parts[0] != 'N/A' else 'N/A'\n",
    "            severity = parts[1] if len(parts) > 1 else ''\n",
    "            \n",
    "            if severity == 'CRITICAL':\n",
    "                cell.fill = critical_fill\n",
    "                cell.font = critical_font\n",
    "            elif severity == 'HIGH':\n",
    "                cell.fill = high_fill\n",
    "                cell.font = high_font\n",
    "            elif severity == 'MEDIUM':\n",
    "                cell.fill = medium_fill\n",
    "                cell.font = medium_font\n",
    "        elif value == 'N/A':\n",
    "            cell.value = 'N/A'\n",
    "            cell.fill = na_fill\n",
    "        else:\n",
    "            cell.value = value\n",
    "            # Color priority column\n",
    "            if col_name == 'PRIORITY':\n",
    "                if value == 'CRITICAL':\n",
    "                    cell.fill = critical_fill\n",
    "                    cell.font = critical_font\n",
    "                elif value == 'HIGH':\n",
    "                    cell.fill = high_fill\n",
    "\n",
    "# Adjust column widths\n",
    "ws.column_dimensions['A'].width = 15\n",
    "ws.column_dimensions['B'].width = 25\n",
    "ws.column_dimensions['C'].width = 12\n",
    "ws.column_dimensions['D'].width = 12\n",
    "ws.column_dimensions['E'].width = 12\n",
    "for col_idx in range(6, len(columns) + 1):\n",
    "    ws.column_dimensions[get_column_letter(col_idx)].width = 10\n",
    "\n",
    "# Freeze panes\n",
    "ws.freeze_panes = 'F2'\n",
    "\n",
    "# Add legend sheet\n",
    "ws_legend = wb.create_sheet('Legend')\n",
    "legend_data = [\n",
    "    ['Color', 'Meaning', 'Description'],\n",
    "    ['RED', 'CRITICAL', 'Bulan dengan penurunan >80% atau usage=0 saat baseline tinggi'],\n",
    "    ['ORANGE', 'HIGH', 'Bulan dengan penurunan 70-80% dari baseline'],\n",
    "    ['YELLOW', 'MEDIUM', 'Bulan dengan penurunan 50-70% dari baseline'],\n",
    "    ['GREEN', 'NORMAL', 'Bulan dengan pola normal'],\n",
    "    ['GRAY', 'N/A', 'Belum berlangganan atau sudah berhenti']\n",
    "]\n",
    "for row_idx, row_data in enumerate(legend_data, 1):\n",
    "    for col_idx, value in enumerate(row_data, 1):\n",
    "        cell = ws_legend.cell(row=row_idx, column=col_idx, value=value)\n",
    "        if row_idx == 1:\n",
    "            cell.font = Font(bold=True)\n",
    "        if col_idx == 1 and row_idx > 1:\n",
    "            colors = {'RED': 'FF0000', 'ORANGE': 'FF6600', 'YELLOW': 'FFCC00', 'GREEN': '92D050', 'GRAY': 'D9D9D9'}\n",
    "            cell.fill = PatternFill(start_color=colors.get(value, 'FFFFFF'), fill_type='solid')\n",
    "\n",
    "# Add summary sheet\n",
    "ws_summary = wb.create_sheet('Summary')\n",
    "summary_data = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Total Eligible Customers', len(df_features)],\n",
    "    ['Predicted Fraud', pred_fraud],\n",
    "    ['Known Fraud (Temuan)', total],\n",
    "    ['Known Fraud Detected', detected],\n",
    "    ['Recall Rate', f\"{recall:.1f}%\"],\n",
    "    ['CRITICAL Priority', (df_features['priority'] == 'CRITICAL').sum()],\n",
    "    ['HIGH Priority', (df_features['priority'] == 'HIGH').sum()],\n",
    "    ['MEDIUM Priority', (df_features['priority'] == 'MEDIUM').sum()],\n",
    "    ['Customers with Theft Months', len(df_theft)]\n",
    "]\n",
    "for row_idx, row_data in enumerate(summary_data, 1):\n",
    "    for col_idx, value in enumerate(row_data, 1):\n",
    "        cell = ws_summary.cell(row=row_idx, column=col_idx, value=value)\n",
    "        if row_idx == 1:\n",
    "            cell.font = Font(bold=True)\n",
    "            cell.fill = header_fill\n",
    "            cell.font = header_font\n",
    "\n",
    "# Save workbook\n",
    "wb.save(output_file)\n",
    "print(f\"\\n[OK] File saved: {output_file}\")\n",
    "print(f\"\\nSheets:\")\n",
    "print(f\"  1. Theft_Detection_Matrix - Main results with color-coded theft months\")\n",
    "print(f\"  2. Legend - Color meaning explanation\")\n",
    "print(f\"  3. Summary - Key metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08b022e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models...\n",
      "[OK] Models saved to ../models/\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 16: SAVE MODELS\n",
    "# =============================================================================\n",
    "print(\"Saving models...\")\n",
    "\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "joblib.dump(rf_model, '../models/random_forest.joblib')\n",
    "joblib.dump(xgb_model, '../models/xgboost.joblib')\n",
    "joblib.dump(scaler, '../models/scaler.joblib')\n",
    "joblib.dump(le_ue, '../models/label_encoder_ue.joblib')\n",
    "joblib.dump(iso_forest, '../models/isolation_forest.joblib')\n",
    "\n",
    "# Save metadata\n",
    "import json\n",
    "metadata = {\n",
    "    'features': all_feature_cols,\n",
    "    'n_features': len(all_feature_cols),\n",
    "    'min_active_months': MIN_ACTIVE_MONTHS,\n",
    "    'training_samples': len(df_train),\n",
    "    'known_fraud_count': len(df_known_fraud),\n",
    "    'recall': recall\n",
    "}\n",
    "with open('../models/metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"[OK] Models saved to ../models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e6ef444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                    FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "DATA:\n",
      "  - Total pelanggan: 152,974\n",
      "  - Eligible untuk prediksi: 132,738\n",
      "  - Tidak eligible (< 12 bulan): 20,236\n",
      "\n",
      "FEATURES:\n",
      "  - Basic Features: 13\n",
      "  - Temporal Features: 14\n",
      "  - Total: 27 features\n",
      "\n",
      "MODEL PERFORMANCE:\n",
      "  - Known Fraud: 41\n",
      "  - Detected: 41\n",
      "  - Recall: 100.0%\n",
      "\n",
      "PREDICTIONS:\n",
      "  - CRITICAL (prob >= 70%): 23,151\n",
      "  - HIGH (prob >= 50%): 4,605\n",
      "  - MEDIUM (prob >= 30%): 2,190\n",
      "\n",
      "OUTPUT FILES:\n",
      "  - results/electricity_theft_detection_results.xlsx\n",
      "    (dengan color coding: MERAH=theft bulan critical)\n",
      "\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 17: FINAL SUMMARY\n",
    "# =============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"                    FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "DATA:\n",
    "  - Total pelanggan: {len(df_history):,}\n",
    "  - Eligible untuk prediksi: {len(df_features):,}\n",
    "  - Tidak eligible (< 12 bulan): {len(df_not_eligible):,}\n",
    "\n",
    "FEATURES:\n",
    "  - Basic Features: 13\n",
    "  - Temporal Features: 14\n",
    "  - Total: 27 features\n",
    "\n",
    "MODEL PERFORMANCE:\n",
    "  - Known Fraud: {total}\n",
    "  - Detected: {detected}\n",
    "  - Recall: {recall:.1f}%\n",
    "\n",
    "PREDICTIONS:\n",
    "  - CRITICAL (prob >= 70%): {(df_features['priority'] == 'CRITICAL').sum():,}\n",
    "  - HIGH (prob >= 50%): {(df_features['priority'] == 'HIGH').sum():,}\n",
    "  - MEDIUM (prob >= 30%): {(df_features['priority'] == 'MEDIUM').sum():,}\n",
    "\n",
    "OUTPUT FILES:\n",
    "  - results/electricity_theft_detection_results.xlsx\n",
    "    (dengan color coding: MERAH=theft bulan critical)\n",
    "\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
