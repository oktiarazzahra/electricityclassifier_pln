{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868471eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded ✓\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [1] IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "print(\"Libraries loaded ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da1ce86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History : 152,974 rows\n",
      "Temuan  : 177 rows (fraud)\n",
      "Normal  : 10,647 rows (normal)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [2] LOAD DATA\n",
    "# ============================================================================\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "df_history = pd.read_excel('data/Histori Pemakaian Pelanggan_rev0A..xlsx')\n",
    "df_temuan = pd.read_excel('data/Temuan_rev0A.xlsx')\n",
    "df_normal = pd.read_excel('data/Normal.xlsx')\n",
    "\n",
    "print(f\"History : {len(df_history):,} rows\")\n",
    "print(f\"Temuan  : {len(df_temuan):,} rows (fraud)\")\n",
    "print(f\"Normal  : {len(df_normal):,} rows (normal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d2274752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DUPLICATE CHECK\n",
      "======================================================================\n",
      "\n",
      "[TEMUAN] Duplicates (IE+UE):\n",
      "  Total duplicate rows: 2\n",
      "\n",
      "  Duplicate data:\n",
      "              IE       UE TARIF  DAYA TGL TEMUAN\n",
      "AAAAAwICAgAFBwEE AAAAAwI=    B1  1300 2024-05-22\n",
      "AAAAAwICAgAFBwEE AAAAAwI=    B1  1300 2025-09-03\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[NORMAL] Duplicates (IE only):\n",
      "  Total duplicate rows: 1345\n",
      "\n",
      "  Duplicate data (showing first 20):\n",
      "              IE TARIF  DAYA\n",
      "AAAAAAACAgACAQQG    R1  2200\n",
      "AAAAAAACAgACAQQG    R1  2200\n",
      "AAAAAAACAgACAwoL    R1   450\n",
      "AAAAAAACAgACAwoL    R1   450\n",
      "AAAAAAACAgEACgIA    R1   450\n",
      "AAAAAAACAgEACgIA    R1   450\n",
      "AAAAAAACAgICBwME    R1   450\n",
      "AAAAAAACAgICBwME    R1   450\n",
      "AAAAAAACAgIDAQQH    B1  1300\n",
      "AAAAAAACAgIDAQQH    B1  1300\n",
      "AAAAAAACAgIEBwID    R1   450\n",
      "AAAAAAACAgIEBwID    R1   450\n",
      "AAAAAAACAgILAAYC    R1   450\n",
      "AAAAAAACAgILAAYC    R1   450\n",
      "AAAAAAACAgMBAAYC   R1M   900\n",
      "AAAAAAACAgMBAAYC   R1M   900\n",
      "AAAAAAACAgMCAgMC    B2 13200\n",
      "AAAAAAACAgMCAgMC    R1   450\n",
      "AAAAAAACAgMCAgMC    B1  1300\n",
      "AAAAAAACAgMCAgMC   R1M   900\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[HISTORY] Duplicates (IE+UE):\n",
      "  Total duplicate rows: 26\n",
      "\n",
      "  Duplicate data (showing first 20):\n",
      "              IE       UE TARIF  DAYA\n",
      "AAAAAAMDAQsAAwAK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsAAwAK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsBAQsK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsBAQsK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsBBAcC AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsBBAcC AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsDBAIA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsDBAIA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsEAgoL AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsEAgoL AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsEAwMB AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsEAwMB AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGAAYA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGAAYA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGAwML AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGAwML AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGBAUK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGBAUK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsHBgIA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsHBgIA AAAAAAM=   NaN   NaN\n",
      "\n",
      "======================================================================\n",
      "OVERLAP CHECK: Fraud vs Normal (IE level)\n",
      "  IE in both Fraud & Normal: 10\n",
      "  Note: FRAUD label wins for overlapping IE\n",
      "  Overlapping IE samples: ['AAAAAgICBQUHBQoA', 'AAAAAgICBgYFAwUL', 'AAAAAgMCBQQFCgEE', 'AAAAAgMCAAMACgQD', 'AAAAAgMCBQICAAEK']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [3] CHECK DUPLICATES\n",
    "# ============================================================================\n",
    "# For Temuan & History: Check IE+UE (same customer + same unit)\n",
    "# For Normal: Only check IE (no UE column)\n",
    "# Note: Same IE but different UE is NOT a duplicate (same customer, different unit)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DUPLICATE CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TEMUAN: Check IE+UE combination\n",
    "dup_temuan = df_temuan[df_temuan.duplicated(subset=['IE', 'UE'], keep=False)]\n",
    "print(\"\\n[TEMUAN] Duplicates (IE+UE):\")\n",
    "print(f\"  Total duplicate rows: {len(dup_temuan)}\")\n",
    "if len(dup_temuan) > 0:\n",
    "    print(f\"\\n  Duplicate data:\")\n",
    "    print(dup_temuan[['IE', 'UE', 'TARIF', 'DAYA', 'TGL TEMUAN']].sort_values(['IE', 'UE']).to_string(index=False))\n",
    "else:\n",
    "    print(\"  ✓ No duplicates\")\n",
    "\n",
    "# NORMAL: Check IE only (no UE column)\n",
    "dup_normal = df_normal[df_normal.duplicated(subset=['IE'], keep=False)]\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"[NORMAL] Duplicates (IE only):\")\n",
    "print(f\"  Total duplicate rows: {len(dup_normal)}\")\n",
    "if len(dup_normal) > 0:\n",
    "    print(f\"\\n  Duplicate data (showing first 20):\")\n",
    "    print(dup_normal[['IE', 'TARIF', 'DAYA']].sort_values(['IE']).head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"  ✓ No duplicates\")\n",
    "\n",
    "# HISTORY: Check IE+UE combination\n",
    "dup_history = df_history[df_history.duplicated(subset=['IE', 'UE'], keep=False)]\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"[HISTORY] Duplicates (IE+UE):\")\n",
    "print(f\"  Total duplicate rows: {len(dup_history)}\")\n",
    "if len(dup_history) > 0:\n",
    "    print(f\"\\n  Duplicate data (showing first 20):\")\n",
    "    cols_to_show = ['IE', 'UE', 'TARIF', 'DAYA']\n",
    "    print(dup_history[cols_to_show].sort_values(['IE', 'UE']).head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"  ✓ No duplicates\")\n",
    "\n",
    "# Check overlap between Fraud and Normal (IE level)\n",
    "ie_fraud = set(df_temuan['IE'])\n",
    "ie_normal = set(df_normal['IE'])\n",
    "overlap = ie_fraud & ie_normal\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"OVERLAP CHECK: Fraud vs Normal (IE level)\")\n",
    "print(f\"  IE in both Fraud & Normal: {len(overlap)}\")\n",
    "if len(overlap) > 0:\n",
    "    print(f\"  Note: FRAUD label wins for overlapping IE\")\n",
    "    print(f\"  Overlapping IE samples: {list(overlap)[:5]}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb8a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning duplicates...\n",
      "\n",
      "[TEMUAN]\n",
      "  Before: 177 rows\n",
      "  After : 176 rows\n",
      "  Removed: 1 duplicates\n",
      "\n",
      "[NORMAL]\n",
      "  Before: 10,647 rows\n",
      "  After : 9,920 rows\n",
      "  Removed: 727 duplicates\n",
      "\n",
      "[HISTORY]\n",
      "  Before: 152,974 rows\n",
      "  After : 152,961 rows\n",
      "  Removed: 13 duplicates\n",
      "\n",
      "[OVERLAP HANDLING]\n",
      "  Normal IE overlapping with Fraud: 10\n",
      "  Final Normal count: 9,910\n",
      "\n",
      "✓ Cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [4] CLEAN DUPLICATES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Cleaning duplicates...\")\n",
    "\n",
    "# TEMUAN: Drop duplicates by IE+UE\n",
    "before_temuan = len(df_temuan)\n",
    "df_temuan_clean = df_temuan.drop_duplicates(subset=['IE', 'UE'], keep='last')\n",
    "print(f\"\\n[TEMUAN]\")\n",
    "print(f\"  Before: {before_temuan:,} rows\")\n",
    "print(f\"  After : {len(df_temuan_clean):,} rows\")\n",
    "print(f\"  Removed: {before_temuan - len(df_temuan_clean):,} duplicates\")\n",
    "\n",
    "# NORMAL: Drop duplicates by IE only\n",
    "before_normal = len(df_normal)\n",
    "df_normal_clean = df_normal.drop_duplicates(subset=['IE'], keep='first')\n",
    "print(f\"\\n[NORMAL]\")\n",
    "print(f\"  Before: {before_normal:,} rows\")\n",
    "print(f\"  After : {len(df_normal_clean):,} rows\")\n",
    "print(f\"  Removed: {before_normal - len(df_normal_clean):,} duplicates\")\n",
    "\n",
    "# HISTORY: Drop duplicates by IE+UE\n",
    "before_history = len(df_history)\n",
    "df_history_clean = df_history.drop_duplicates(subset=['IE', 'UE'], keep='first')\n",
    "print(f\"\\n[HISTORY]\")\n",
    "print(f\"  Before: {before_history:,} rows\")\n",
    "print(f\"  After : {len(df_history_clean):,} rows\")\n",
    "print(f\"  Removed: {before_history - len(df_history_clean):,} duplicates\")\n",
    "\n",
    "# Remove overlap from Normal (FRAUD wins)\n",
    "ie_fraud = set(df_temuan_clean['IE'])\n",
    "before_overlap = len(df_normal_clean)\n",
    "df_normal_clean = df_normal_clean[~df_normal_clean['IE'].isin(ie_fraud)]\n",
    "\n",
    "print(f\"\\n[OVERLAP HANDLING]\")\n",
    "print(f\"  Normal IE overlapping with Fraud: {before_overlap - len(df_normal_clean):,}\")\n",
    "print(f\"  Final Normal count: {len(df_normal_clean):,}\")\n",
    "\n",
    "print(\"\\n✓ Cleaning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b95631b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels assigned:\n",
      "  Fraud (1)   : 176\n",
      "  Normal (0)  : 9,889\n",
      "  Unknown (-1): 142,883\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [5] LABELING\n",
    "# ============================================================================\n",
    "# Label: 1 = Fraud (Temuan), 0 = Normal, -1 = Unknown\n",
    "ie_fraud = set(df_temuan_clean['IE'])\n",
    "ie_normal = set(df_normal_clean['IE'])\n",
    "\n",
    "def assign_label(ie):\n",
    "    if ie in ie_fraud:\n",
    "        return 1\n",
    "    elif ie in ie_normal:\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "df_history_clean['label'] = df_history_clean['IE'].apply(assign_label)\n",
    "\n",
    "label_counts = df_history_clean['label'].value_counts()\n",
    "print(\"Labels assigned:\")\n",
    "print(f\"  Fraud (1)   : {label_counts.get(1, 0):,}\")\n",
    "print(f\"  Normal (0)  : {label_counts.get(0, 0):,}\")\n",
    "print(f\"  Unknown (-1): {label_counts.get(-1, 0):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5e17ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Verification:\n",
      "  Total unique fraud (Temuan)    : 176\n",
      "  Found in History               : 168\n",
      "  NOT in History (missing)       : 8\n",
      "\n",
      "Missing fraud IE samples: ['AAAAAgIHAgEDBQoA', 'AAAAAgIHAgIDBwEG', 'AAAAAgIDAgQCAQUE', 'AAAAAgMCBQYCCgAK', 'AAAAAgMCBgYDBgQL']\n",
      "\n",
      "Actual fraud labeled in dataset  : 168\n",
      "\n",
      "Temuan date range:\n",
      "  First: 2024-01-02 00:00:00\n",
      "  Last : 2025-12-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [5B] VERIFY FRAUD DATA - Check missing fraud cases\n",
    "# ============================================================================\n",
    "# Check which fraud IE are NOT in history\n",
    "ie_temuan_all = set(df_temuan_clean['IE'])\n",
    "ie_history_all = set(df_history_clean['IE'])\n",
    "\n",
    "# IE fraud yang tidak ada di history\n",
    "missing_fraud = ie_temuan_all - ie_history_all\n",
    "found_fraud = ie_temuan_all & ie_history_all\n",
    "\n",
    "print(\"Fraud Verification:\")\n",
    "print(f\"  Total unique fraud (Temuan)    : {len(ie_temuan_all):,}\")\n",
    "print(f\"  Found in History               : {len(found_fraud):,}\")\n",
    "print(f\"  NOT in History (missing)       : {len(missing_fraud):,}\")\n",
    "\n",
    "if len(missing_fraud) > 0:\n",
    "    print(f\"\\nMissing fraud IE samples: {list(missing_fraud)[:5]}\")\n",
    "\n",
    "# Check actual labeled fraud in df_history_clean\n",
    "actual_fraud = (df_history_clean['label'] == 1).sum()\n",
    "print(f\"\\nActual fraud labeled in dataset  : {actual_fraud:,}\")\n",
    "\n",
    "# Verify temuan dates - which months have fraud pattern\n",
    "print(\"\\nTemuan date range:\")\n",
    "print(f\"  First: {df_temuan['TGL TEMUAN'].min()}\")\n",
    "print(f\"  Last : {df_temuan['TGL TEMUAN'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e60dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "  ['UE', 'IE', 'TARIF', 'DAYA', datetime.datetime(2021, 3, 1, 0, 0)] ... (64 total)\n",
      "\n",
      "Data shape: (152948, 64)\n",
      "\n",
      "Data types:\n",
      "float64    60\n",
      "object      3\n",
      "int64       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "TARIF                  13412\n",
      "DAYA                   13412\n",
      "2021-03-01 00:00:00     8944\n",
      "2021-04-01 00:00:00     8881\n",
      "2021-05-01 00:00:00     8826\n",
      "                       ...  \n",
      "2025-09-01 00:00:00    16270\n",
      "2025-10-01 00:00:00    15778\n",
      "2025-11-01 00:00:00    15093\n",
      "2025-12-01 00:00:00    14724\n",
      "2026-01-01 00:00:00    14307\n",
      "Length: 61, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [6] DATA INSPECTION\n",
    "# ============================================================================\n",
    "print(\"Column names:\")\n",
    "print(f\"  {list(df_history_clean.columns[:5])} ... ({len(df_history_clean.columns)} total)\")\n",
    "\n",
    "print(f\"\\nData shape: {df_history_clean.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_history_clean.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df_history_clean.isnull().sum()\n",
    "missing_cols = missing[missing > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(missing_cols)\n",
    "else:\n",
    "    print(\"  No missing values ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93721431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date columns: 59\n",
      "  First: 2021-03-01 00:00:00\n",
      "  Last : 2026-01-01 00:00:00\n",
      "  Range: 59 months\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [7] EXTRACT DATE COLUMNS\n",
    "# ============================================================================\n",
    "non_date_cols = ['IE', 'UE', 'TARIF', 'DAYA', 'label']\n",
    "date_columns = [col for col in df_history_clean.columns if col not in non_date_cols]\n",
    "date_columns = sorted(date_columns)\n",
    "\n",
    "print(f\"Date columns: {len(date_columns)}\")\n",
    "print(f\"  First: {date_columns[0]}\")\n",
    "print(f\"  Last : {date_columns[-1]}\")\n",
    "print(f\"  Range: {len(date_columns)} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a29bf18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values filled: 26824 remaining\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [8] HANDLE MISSING VALUES\n",
    "# ============================================================================\n",
    "# Fill NaN in usage columns with 0\n",
    "df_history_clean[date_columns] = df_history_clean[date_columns].fillna(0)\n",
    "\n",
    "# Check result\n",
    "remaining_na = df_history_clean.isnull().sum().sum()\n",
    "print(f\"Missing values filled: {remaining_na} remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8758d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers with N/A in last 3 months (unsubscribed): 13,904\n",
      "Removed: 13,904 unsubscribed customers\n",
      "Remaining: 139,044 active customers\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [9] FILTER INACTIVE CUSTOMERS (N/A at end = Unsubscribed)\n",
    "# ============================================================================\n",
    "# Check last 3 months - if all NaN/0, likely unsubscribed\n",
    "last_3_cols = date_columns[-3:]\n",
    "\n",
    "def is_unsubscribed(row):\n",
    "    \"\"\"Customer with NaN at the end = no longer subscribed\"\"\"\n",
    "    last_3_vals = row[last_3_cols].values\n",
    "    # Check if ALL last 3 months are NaN (before fillna)\n",
    "    return pd.isna(last_3_vals).all()\n",
    "\n",
    "# Need to check BEFORE fillna - reload and check\n",
    "df_temp = df_history.drop_duplicates(subset=['IE'], keep='first')\n",
    "\n",
    "# Check unsubscribed pattern\n",
    "unsubscribed_mask = df_temp[last_3_cols].isna().all(axis=1)\n",
    "unsubscribed_count = unsubscribed_mask.sum()\n",
    "\n",
    "print(f\"Customers with N/A in last 3 months (unsubscribed): {unsubscribed_count:,}\")\n",
    "\n",
    "# Filter out unsubscribed from df_history_clean\n",
    "unsubscribed_ie = set(df_temp[unsubscribed_mask]['IE'])\n",
    "before = len(df_history_clean)\n",
    "df_history_clean = df_history_clean[~df_history_clean['IE'].isin(unsubscribed_ie)]\n",
    "after = len(df_history_clean)\n",
    "\n",
    "print(f\"Removed: {before - after:,} unsubscribed customers\")\n",
    "print(f\"Remaining: {after:,} active customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dacb46ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage Statistics:\n",
      "  Min  : 0.00\n",
      "  Max  : 24229392.00\n",
      "  Mean : 595.21\n",
      "  Std  : 20718.42\n",
      "\n",
      "Fraud:\n",
      "  Mean: 218.87\n",
      "  Std : 244.10\n",
      "\n",
      "Normal:\n",
      "  Mean: 304.08\n",
      "  Std : 776.66\n",
      "\n",
      "Unknown:\n",
      "  Mean: 617.83\n",
      "  Std : 21503.14\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [9] BASIC STATISTICS\n",
    "# ============================================================================\n",
    "usage_matrix = df_history_clean[date_columns].values\n",
    "\n",
    "print(\"Usage Statistics:\")\n",
    "print(f\"  Min  : {usage_matrix.min():.2f}\")\n",
    "print(f\"  Max  : {usage_matrix.max():.2f}\")\n",
    "print(f\"  Mean : {usage_matrix.mean():.2f}\")\n",
    "print(f\"  Std  : {usage_matrix.std():.2f}\")\n",
    "\n",
    "# Statistics by label\n",
    "for label in [1, 0, -1]:\n",
    "    label_name = {1: 'Fraud', 0: 'Normal', -1: 'Unknown'}[label]\n",
    "    mask = df_history_clean['label'] == label\n",
    "    if mask.sum() > 0:\n",
    "        values = df_history_clean.loc[mask, date_columns].values\n",
    "        print(f\"\\n{label_name}:\")\n",
    "        print(f\"  Mean: {values.mean():.2f}\")\n",
    "        print(f\"  Std : {values.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a23ba964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extractor Class defined ✓\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [15] ADVANCED FEATURE EXTRACTOR CLASS (21 Features)\n",
    "# ============================================================================\n",
    "from scipy import stats\n",
    "\n",
    "class ElectricityTheftFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extract 21 optimized features for electricity theft detection\n",
    "    Based on 3 fraud patterns: Zero fraud, Gradual decline, Sudden spike\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df_normal, date_columns, tarif_col='TARIF', ue_col='UE', daya_col='DAYA'):\n",
    "        self.df_normal = df_normal\n",
    "        self.date_columns = date_columns\n",
    "        self.tarif_col = tarif_col\n",
    "        self.ue_col = ue_col\n",
    "        self.daya_col = daya_col\n",
    "        \n",
    "        # Build benchmarks from NORMAL data\n",
    "        self._build_benchmarks()\n",
    "        \n",
    "    def _build_benchmarks(self):\n",
    "        \"\"\"Build statistical benchmarks from normal customers\"\"\"\n",
    "        print(\"Building benchmarks from NORMAL customers...\")\n",
    "        \n",
    "        # Group by TARIF\n",
    "        self.tarif_stats = {}\n",
    "        for tarif in self.df_normal[self.tarif_col].unique():\n",
    "            mask = self.df_normal[self.tarif_col] == tarif\n",
    "            usage = self.df_normal.loc[mask, self.date_columns].values.flatten()\n",
    "            usage = usage[usage > 0]  # Only non-zero\n",
    "            \n",
    "            if len(usage) > 0:\n",
    "                self.tarif_stats[tarif] = {\n",
    "                    'mean': usage.mean(),\n",
    "                    'std': usage.std() + 1e-6,\n",
    "                    'median': np.median(usage)\n",
    "                }\n",
    "        \n",
    "        # Group by UE (neighborhood)\n",
    "        self.ue_stats = {}\n",
    "        for ue in self.df_normal[self.ue_col].unique():\n",
    "            mask = self.df_normal[self.ue_col] == ue\n",
    "            usage = self.df_normal.loc[mask, self.date_columns].values.flatten()\n",
    "            usage = usage[usage > 0]\n",
    "            \n",
    "            if len(usage) > 0:\n",
    "                self.ue_stats[ue] = usage\n",
    "        \n",
    "        print(f\"  Tarif groups: {len(self.tarif_stats)}\")\n",
    "        print(f\"  UE groups: {len(self.ue_stats)}\")\n",
    "    \n",
    "    def extract_features(self, df):\n",
    "        \"\"\"Extract all 21 features from dataframe\"\"\"\n",
    "        print(f\"Extracting 21 features from {len(df):,} customers...\")\n",
    "        \n",
    "        # Reset index untuk mudahkan mapping\n",
    "        df_work = df.reset_index(drop=True)\n",
    "        features = pd.DataFrame(index=df_work.index)\n",
    "        usage_matrix = df_work[self.date_columns].values\n",
    "        \n",
    "        # Comparison Features\n",
    "        print(\"  Extracting comparison features...\")\n",
    "        features['tariff_zscore'] = self._tariff_zscore_vectorized(df_work, usage_matrix)\n",
    "        features['usage_vs_expected'] = self._usage_vs_expected_vectorized(df_work, usage_matrix)\n",
    "        features['neighborhood_percentile'] = self._neighborhood_percentile_vectorized(df_work, usage_matrix)\n",
    "        features['cv'] = self._coefficient_variation(usage_matrix)\n",
    "        features['zero_fraud_score'] = self._zero_fraud_score(usage_matrix)\n",
    "        \n",
    "        # Pattern Detection Features\n",
    "        print(\"  Extracting pattern features...\")\n",
    "        features['plateau_months'] = self._plateau_months(usage_matrix)\n",
    "        features['gradual_decline'] = self._gradual_decline(usage_matrix)\n",
    "        features['max_consecutive_zero'] = self._max_consecutive_zero(usage_matrix)\n",
    "        features['zero_clusters'] = self._zero_clusters(usage_matrix)\n",
    "        features['extreme_spike_count'] = self._extreme_spike_count(usage_matrix)\n",
    "        features['sudden_drop_count'] = self._sudden_drop_count(usage_matrix)\n",
    "        features['consecutive_below_threshold'] = self._consecutive_below_threshold(usage_matrix)\n",
    "        features['drop_recovery_pattern'] = self._drop_recovery_pattern(usage_matrix)\n",
    "        features['variance_spike_count'] = self._variance_spike_count(usage_matrix)\n",
    "        features['max_drop_from_baseline'] = self._max_drop_from_baseline(usage_matrix)\n",
    "        \n",
    "        # Loss & Priority Features\n",
    "        print(\"  Extracting loss & priority features...\")\n",
    "        features['sudden_spike_sustained'] = self._sudden_spike_sustained(usage_matrix)\n",
    "        features['estimated_monthly_loss'] = self._estimated_monthly_loss_vectorized(df_work, usage_matrix)\n",
    "        features['investigation_roi'] = self._investigation_roi(features, df_work)\n",
    "        \n",
    "        # Zero Pattern Features\n",
    "        print(\"  Extracting zero pattern features...\")\n",
    "        features['zero_pattern_legitimacy'] = self._zero_pattern_legitimacy(usage_matrix)\n",
    "        features['zero_to_nonzero_transition'] = self._zero_to_nonzero_transition(usage_matrix)\n",
    "        features['post_zero_spike_severity'] = self._post_zero_spike_severity(usage_matrix)\n",
    "        \n",
    "        # Restore original index\n",
    "        features.index = df.index\n",
    "        \n",
    "        print(f\"✓ Extracted {features.shape[1]} features\")\n",
    "        return features\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Feature Extraction Methods\n",
    "    # =========================================================================\n",
    "    \n",
    "    def _tariff_zscore_vectorized(self, df, usage_matrix):\n",
    "        \"\"\"Z-score vs normal customers with same tariff\"\"\"\n",
    "        result = np.zeros(len(df))\n",
    "        mean_usage = np.array([row[row > 0].mean() if (row > 0).any() else 0 for row in usage_matrix])\n",
    "        \n",
    "        for tarif, stats in self.tarif_stats.items():\n",
    "            mask = df[self.tarif_col] == tarif\n",
    "            if mask.any():\n",
    "                zscore = (mean_usage[mask] - stats['mean']) / stats['std']\n",
    "                result[mask] = zscore\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _usage_vs_expected_vectorized(self, df, usage_matrix):\n",
    "        \"\"\"Ratio actual/expected usage\"\"\"\n",
    "        result = np.ones(len(df))\n",
    "        mean_usage = np.array([row[row > 0].mean() if (row > 0).any() else 0 for row in usage_matrix])\n",
    "        \n",
    "        for tarif, stats in self.tarif_stats.items():\n",
    "            mask = df[self.tarif_col] == tarif\n",
    "            if mask.any():\n",
    "                ratio = mean_usage[mask] / (stats['median'] + 1e-6)\n",
    "                result[mask] = ratio\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _neighborhood_percentile_vectorized(self, df, usage_matrix):\n",
    "        \"\"\"Percentile in same neighborhood (UE)\"\"\"\n",
    "        result = np.full(len(df), 50.0)\n",
    "        mean_usage = np.array([row[row > 0].mean() if (row > 0).any() else 0 for row in usage_matrix])\n",
    "        \n",
    "        for ue, ue_usage in self.ue_stats.items():\n",
    "            mask = df[self.ue_col] == ue\n",
    "            if mask.any():\n",
    "                percentiles = [stats.percentileofscore(ue_usage, val) for val in mean_usage[mask]]\n",
    "                result[mask] = percentiles\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _coefficient_variation(self, usage_matrix):\n",
    "        \"\"\"CV = std / mean (stability)\"\"\"\n",
    "        mean_vals = usage_matrix.mean(axis=1)\n",
    "        std_vals = usage_matrix.std(axis=1)\n",
    "        cv = std_vals / (mean_vals + 1e-6)\n",
    "        return cv\n",
    "    \n",
    "    def _zero_fraud_score(self, usage_matrix):\n",
    "        \"\"\"Composite score for zero detection (0-100)\"\"\"\n",
    "        zero_count = (usage_matrix == 0).sum(axis=1)\n",
    "        zero_pct = zero_count / usage_matrix.shape[1] * 100\n",
    "        \n",
    "        # Consecutive zeros\n",
    "        max_consec_zero = self._max_consecutive_zero(usage_matrix)\n",
    "        \n",
    "        # Score: higher = more suspicious\n",
    "        score = (zero_pct * 0.5) + (max_consec_zero * 5)\n",
    "        return np.clip(score, 0, 100)\n",
    "    \n",
    "    def _plateau_months(self, usage_matrix):\n",
    "        \"\"\"Max months stuck at same value (±5%)\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            row = row[row > 0]  # Only non-zero\n",
    "            if len(row) < 2:\n",
    "                result.append(0)\n",
    "                continue\n",
    "            \n",
    "            max_plateau = 1\n",
    "            current_plateau = 1\n",
    "            \n",
    "            for i in range(1, len(row)):\n",
    "                if abs(row[i] - row[i-1]) / (row[i-1] + 1e-6) <= 0.05:\n",
    "                    current_plateau += 1\n",
    "                    max_plateau = max(max_plateau, current_plateau)\n",
    "                else:\n",
    "                    current_plateau = 1\n",
    "            \n",
    "            result.append(max_plateau)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _gradual_decline(self, usage_matrix):\n",
    "        \"\"\"Binary: decline >20% in 10 months?\"\"\"\n",
    "        result = []\n",
    "        window = 10\n",
    "        \n",
    "        for row in usage_matrix:\n",
    "            row = row[row > 0]\n",
    "            if len(row) < window:\n",
    "                result.append(0)\n",
    "                continue\n",
    "            \n",
    "            max_decline = 0\n",
    "            for i in range(len(row) - window + 1):\n",
    "                start_val = row[i:i+3].mean()  # Average first 3\n",
    "                end_val = row[i+window-3:i+window].mean()  # Average last 3\n",
    "                decline_pct = (start_val - end_val) / (start_val + 1e-6) * 100\n",
    "                max_decline = max(max_decline, decline_pct)\n",
    "            \n",
    "            result.append(1 if max_decline > 20 else 0)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _max_consecutive_zero(self, usage_matrix):\n",
    "        \"\"\"Max consecutive zero months\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            max_zero = 0\n",
    "            current_zero = 0\n",
    "            for val in row:\n",
    "                if val == 0:\n",
    "                    current_zero += 1\n",
    "                    max_zero = max(max_zero, current_zero)\n",
    "                else:\n",
    "                    current_zero = 0\n",
    "            result.append(max_zero)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _zero_clusters(self, usage_matrix):\n",
    "        \"\"\"Count of 2+ consecutive zero clusters\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            clusters = 0\n",
    "            current_zero = 0\n",
    "            for val in row:\n",
    "                if val == 0:\n",
    "                    current_zero += 1\n",
    "                else:\n",
    "                    if current_zero >= 2:\n",
    "                        clusters += 1\n",
    "                    current_zero = 0\n",
    "            if current_zero >= 2:\n",
    "                clusters += 1\n",
    "            result.append(clusters)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _extreme_spike_count(self, usage_matrix):\n",
    "        \"\"\"Count spikes >200% of baseline\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            row = row[row > 0]\n",
    "            if len(row) < 3:\n",
    "                result.append(0)\n",
    "                continue\n",
    "            \n",
    "            baseline = np.median(row[:len(row)//2])\n",
    "            spikes = (row > baseline * 3).sum()\n",
    "            result.append(spikes)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _sudden_drop_count(self, usage_matrix):\n",
    "        \"\"\"Count drops >40% month-to-month\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            row = row[row > 0]\n",
    "            if len(row) < 2:\n",
    "                result.append(0)\n",
    "                continue\n",
    "            \n",
    "            drops = 0\n",
    "            for i in range(1, len(row)):\n",
    "                drop_pct = (row[i-1] - row[i]) / (row[i-1] + 1e-6) * 100\n",
    "                if drop_pct > 40:\n",
    "                    drops += 1\n",
    "            result.append(drops)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _consecutive_below_threshold(self, usage_matrix):\n",
    "        \"\"\"Max months below 70% baseline\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            row = row[row > 0]\n",
    "            if len(row) < 3:\n",
    "                result.append(0)\n",
    "                continue\n",
    "            \n",
    "            baseline = np.median(row[:len(row)//2])\n",
    "            threshold = baseline * 0.7\n",
    "            \n",
    "            max_below = 0\n",
    "            current_below = 0\n",
    "            for val in row:\n",
    "                if val < threshold:\n",
    "                    current_below += 1\n",
    "                    max_below = max(max_below, current_below)\n",
    "                else:\n",
    "                    current_below = 0\n",
    "            result.append(max_below)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _drop_recovery_pattern(self, usage_matrix):\n",
    "        \"\"\"Count drop → recovery patterns\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            row = row[row > 0]\n",
    "            if len(row) < 3:\n",
    "                result.append(0)\n",
    "                continue\n",
    "            \n",
    "            patterns = 0\n",
    "            in_drop = False\n",
    "            \n",
    "            for i in range(1, len(row)):\n",
    "                drop_pct = (row[i-1] - row[i]) / (row[i-1] + 1e-6) * 100\n",
    "                \n",
    "                if drop_pct > 30 and not in_drop:\n",
    "                    in_drop = True\n",
    "                elif in_drop:\n",
    "                    recovery_pct = (row[i] - row[i-1]) / (row[i-1] + 1e-6) * 100\n",
    "                    if recovery_pct > 20:\n",
    "                        patterns += 1\n",
    "                        in_drop = False\n",
    "            \n",
    "            result.append(patterns)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _variance_spike_count(self, usage_matrix):\n",
    "        \"\"\"Count variance increases >2x\"\"\"\n",
    "        result = []\n",
    "        window = 6\n",
    "        \n",
    "        for row in usage_matrix:\n",
    "            row = row[row > 0]\n",
    "            if len(row) < window * 2:\n",
    "                result.append(0)\n",
    "                continue\n",
    "            \n",
    "            spikes = 0\n",
    "            for i in range(len(row) - window):\n",
    "                var1 = np.var(row[i:i+window//2])\n",
    "                var2 = np.var(row[i+window//2:i+window])\n",
    "                if var2 > var1 * 2:\n",
    "                    spikes += 1\n",
    "            \n",
    "            result.append(spikes)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _max_drop_from_baseline(self, usage_matrix):\n",
    "        \"\"\"% max drop from baseline\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            row = row[row > 0]\n",
    "            if len(row) < 3:\n",
    "                result.append(0)\n",
    "                continue\n",
    "            \n",
    "            baseline = np.median(row[:len(row)//2])\n",
    "            min_val = row[len(row)//2:].min()\n",
    "            drop_pct = (baseline - min_val) / (baseline + 1e-6) * 100\n",
    "            result.append(max(0, drop_pct))\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _sudden_spike_sustained(self, usage_matrix):\n",
    "        \"\"\"Binary: spike >50% sustained >6 months\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            row = row[row > 0]\n",
    "            if len(row) < 12:\n",
    "                result.append(0)\n",
    "                continue\n",
    "            \n",
    "            baseline = np.median(row[:len(row)//2])\n",
    "            threshold = baseline * 1.5\n",
    "            \n",
    "            # Check if any 6-month period has sustained spike\n",
    "            sustained = False\n",
    "            for i in range(len(row) - 6):\n",
    "                if (row[i:i+6] > threshold).sum() >= 5:\n",
    "                    sustained = True\n",
    "                    break\n",
    "            \n",
    "            result.append(1 if sustained else 0)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _estimated_monthly_loss_vectorized(self, df, usage_matrix):\n",
    "        \"\"\"Estimated monthly loss in Rupiah\"\"\"\n",
    "        result = np.zeros(len(df))\n",
    "        mean_usage = np.array([row[row > 0].mean() if (row > 0).any() else 0 for row in usage_matrix])\n",
    "        \n",
    "        # Estimate rate (Rp/kWh) based on tariff\n",
    "        rate_map = {'R1': 1500, 'R2': 1800, 'R3': 2000, 'B1': 1400, 'B2': 1600, 'B3': 1800}\n",
    "        \n",
    "        for tarif, stats in self.tarif_stats.items():\n",
    "            mask = df[self.tarif_col] == tarif\n",
    "            if mask.any():\n",
    "                expected = stats['median']\n",
    "                loss_kwh = np.maximum(0, expected - mean_usage[mask])\n",
    "                \n",
    "                # Get rate\n",
    "                tarif_str = str(tarif)[:2]\n",
    "                rate = rate_map.get(tarif_str, 1500)\n",
    "                \n",
    "                result[mask] = loss_kwh * rate\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _investigation_roi(self, features, df):\n",
    "        \"\"\"Priority score for investigation (0-100)\"\"\"\n",
    "        # Weights for ROI\n",
    "        w_loss = 0.3\n",
    "        w_zero_score = 0.2\n",
    "        w_patterns = 0.2\n",
    "        w_decline = 0.15\n",
    "        w_spike = 0.15\n",
    "        \n",
    "        # Normalize components\n",
    "        loss_norm = (features['estimated_monthly_loss'] / (features['estimated_monthly_loss'].max() + 1)) * 100\n",
    "        zero_norm = features['zero_fraud_score']\n",
    "        pattern_norm = (features['sudden_drop_count'] + features['drop_recovery_pattern']) * 10\n",
    "        decline_norm = features['gradual_decline'] * 100\n",
    "        spike_norm = features['sudden_spike_sustained'] * 100\n",
    "        \n",
    "        roi = (w_loss * loss_norm + \n",
    "               w_zero_score * zero_norm + \n",
    "               w_patterns * pattern_norm + \n",
    "               w_decline * decline_norm + \n",
    "               w_spike * spike_norm)\n",
    "        \n",
    "        return np.clip(roi, 0, 100)\n",
    "    \n",
    "    def _zero_pattern_legitimacy(self, usage_matrix):\n",
    "        \"\"\"Score 0-1: is zero pattern legitimate?\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            # Legitimate zeros: evenly distributed, not clustered\n",
    "            zero_mask = (row == 0)\n",
    "            zero_count = zero_mask.sum()\n",
    "            \n",
    "            if zero_count == 0:\n",
    "                result.append(1.0)\n",
    "                continue\n",
    "            \n",
    "            # Check distribution\n",
    "            n_months = len(row)\n",
    "            expected_gap = n_months / (zero_count + 1)\n",
    "            \n",
    "            # Find actual gaps\n",
    "            zero_indices = np.where(zero_mask)[0]\n",
    "            if len(zero_indices) < 2:\n",
    "                legitimacy = 0.5\n",
    "            else:\n",
    "                gaps = np.diff(zero_indices)\n",
    "                gap_std = gaps.std()\n",
    "                legitimacy = 1 / (1 + gap_std / (expected_gap + 1))\n",
    "            \n",
    "            result.append(legitimacy)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _zero_to_nonzero_transition(self, usage_matrix):\n",
    "        \"\"\"Count 0 → non-zero transitions\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            transitions = 0\n",
    "            for i in range(1, len(row)):\n",
    "                if row[i-1] == 0 and row[i] > 0:\n",
    "                    transitions += 1\n",
    "            result.append(transitions)\n",
    "        return np.array(result)\n",
    "    \n",
    "    def _post_zero_spike_severity(self, usage_matrix):\n",
    "        \"\"\"Max recovery ratio after zero\"\"\"\n",
    "        result = []\n",
    "        for row in usage_matrix:\n",
    "            max_ratio = 0\n",
    "            for i in range(1, len(row) - 1):\n",
    "                if row[i] == 0 and row[i+1] > 0:\n",
    "                    if i > 0 and row[i-1] > 0:\n",
    "                        ratio = row[i+1] / (row[i-1] + 1e-6)\n",
    "                        max_ratio = max(max_ratio, ratio)\n",
    "            result.append(max_ratio)\n",
    "        return np.array(result)\n",
    "\n",
    "print(\"Feature Extractor Class defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "caa63c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building benchmarks from NORMAL customers...\n",
      "  Tarif groups: 11\n",
      "  UE groups: 6\n",
      "Extracting 21 features from 139,044 customers...\n",
      "  [TIER S] Universal features...\n",
      "  [TIER A] Pattern detection...\n",
      "  [TIER B] Tapping & ROI...\n",
      "  [TIER C] Enhanced zero detection...\n",
      "✓ Extracted 21 features\n",
      "\n",
      "✓ Feature extraction complete!\n",
      "  Shape: (139044, 26)\n",
      "  Features: ['tariff_zscore', 'usage_vs_expected', 'neighborhood_percentile', 'cv', 'zero_fraud_score', 'plateau_months', 'gradual_decline', 'max_consecutive_zero', 'zero_clusters', 'extreme_spike_count', 'sudden_drop_count', 'consecutive_below_threshold', 'drop_recovery_pattern', 'variance_spike_count', 'max_drop_from_baseline', 'sudden_spike_sustained', 'estimated_monthly_loss', 'investigation_roi', 'zero_pattern_legitimacy', 'zero_to_nonzero_transition', 'post_zero_spike_severity']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [16] EXTRACT FEATURES FROM ALL CUSTOMERS\n",
    "# ============================================================================\n",
    "# Get NORMAL customers from history (label=0) for benchmarking\n",
    "df_normal_for_benchmark = df_history_clean[df_history_clean['label'] == 0].copy()\n",
    "\n",
    "# Initialize feature extractor with NORMAL benchmark\n",
    "extractor = ElectricityTheftFeatureExtractor(\n",
    "    df_normal=df_normal_for_benchmark,\n",
    "    date_columns=date_columns,\n",
    "    tarif_col='TARIF',\n",
    "    ue_col='UE',\n",
    "    daya_col='DAYA'\n",
    ")\n",
    "\n",
    "# Extract features from all customers\n",
    "df_features = extractor.extract_features(df_history_clean)\n",
    "\n",
    "# Combine with original data\n",
    "df_features = pd.concat([\n",
    "    df_history_clean[['IE', 'UE', 'TARIF', 'DAYA', 'label']],\n",
    "    df_features\n",
    "], axis=1)\n",
    "\n",
    "print(f\"\\n✓ Feature extraction complete!\")\n",
    "print(f\"  Shape: {df_features.shape}\")\n",
    "print(f\"  Features: {df_features.columns.tolist()[5:]}\")  # Skip metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "48b7aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features: 21\n",
      "\n",
      "Features: ['tariff_zscore', 'usage_vs_expected', 'neighborhood_percentile', 'cv', 'zero_fraud_score', 'plateau_months', 'gradual_decline', 'max_consecutive_zero', 'zero_clusters', 'extreme_spike_count', 'sudden_drop_count', 'consecutive_below_threshold', 'drop_recovery_pattern', 'variance_spike_count', 'max_drop_from_baseline', 'sudden_spike_sustained', 'estimated_monthly_loss', 'investigation_roi', 'zero_pattern_legitimacy', 'zero_to_nonzero_transition', 'post_zero_spike_severity']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [17] FEATURE COLUMNS (21 Features)\n",
    "# ============================================================================\n",
    "feature_cols = [\n",
    "    # Comparison Features (5)\n",
    "    'tariff_zscore', 'usage_vs_expected', 'neighborhood_percentile', 'cv', 'zero_fraud_score',\n",
    "    \n",
    "    # Pattern Detection Features (10)\n",
    "    'plateau_months', 'gradual_decline', 'max_consecutive_zero', 'zero_clusters',\n",
    "    'extreme_spike_count', 'sudden_drop_count', 'consecutive_below_threshold',\n",
    "    'drop_recovery_pattern', 'variance_spike_count', 'max_drop_from_baseline',\n",
    "    \n",
    "    # Loss & Priority Features (3)\n",
    "    'sudden_spike_sustained', 'estimated_monthly_loss', 'investigation_roi',\n",
    "    \n",
    "    # Zero Pattern Features (3)\n",
    "    'zero_pattern_legitimacy', 'zero_to_nonzero_transition', 'post_zero_spike_severity'\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeatures: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8c4e2314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE EXTRACTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data Overview:\n",
      "  Total customers  : 139,044\n",
      "  - Fraud          : 168\n",
      "  - Normal         : 9,810\n",
      "  - Unknown        : 129,066\n",
      "\n",
      "Features Extracted: 21 features\n",
      "  DataFrame shape  : (139044, 26)\n",
      "\n",
      "Top 5 by Investigation ROI:\n",
      "              IE TARIF  label  investigation_roi  zero_fraud_score\n",
      "AAAAAAEDAgADCgAB    B1     -1              100.0         60.169492\n",
      "AAAAAQACAgMCAAIC    R1     -1              100.0          6.694915\n",
      "AAAAAgMCAAcHAQsG    B2     -1              100.0         23.389831\n",
      "AAAAAgMCBgYKAgEK    P1     -1              100.0          0.000000\n",
      "AAAAAgMCBAcCAQIC    R1     -1              100.0          0.000000\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [18] FEATURE EXTRACTION SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE EXTRACTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nData Overview:\")\n",
    "print(f\"  Total customers  : {len(df_features):,}\")\n",
    "print(f\"  - Fraud          : {(df_features['label'] == 1).sum():,}\")\n",
    "print(f\"  - Normal         : {(df_features['label'] == 0).sum():,}\")\n",
    "print(f\"  - Unknown        : {(df_features['label'] == -1).sum():,}\")\n",
    "\n",
    "print(f\"\\nFeatures Extracted: {len(feature_cols)} features\")\n",
    "print(f\"  DataFrame shape  : {df_features.shape}\")\n",
    "\n",
    "print(f\"\\nTop 5 by Investigation ROI:\")\n",
    "preview_cols = ['IE', 'TARIF', 'label', 'investigation_roi', 'zero_fraud_score']\n",
    "top_roi = df_features.nlargest(5, 'investigation_roi')[preview_cols]\n",
    "print(top_roi.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a3e05",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
