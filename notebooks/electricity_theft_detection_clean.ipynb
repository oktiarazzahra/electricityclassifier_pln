{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868471eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded âœ“\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [1] IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "print(\"Libraries loaded âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da1ce86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History : 152,974 rows\n",
      "Temuan  : 177 rows (fraud)\n",
      "Normal  : 10,647 rows (normal)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [2] LOAD DATA\n",
    "# ============================================================================\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "df_history = pd.read_excel('data/Histori Pemakaian Pelanggan_rev0A..xlsx')\n",
    "df_temuan = pd.read_excel('data/Temuan_rev0A.xlsx')\n",
    "df_normal = pd.read_excel('data/Normal.xlsx')\n",
    "\n",
    "print(f\"History : {len(df_history):,} rows\")\n",
    "print(f\"Temuan  : {len(df_temuan):,} rows (fraud)\")\n",
    "print(f\"Normal  : {len(df_normal):,} rows (normal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d2274752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DUPLICATE CHECK\n",
      "======================================================================\n",
      "\n",
      "[TEMUAN] Duplicates (IE+UE):\n",
      "  Total duplicate rows: 2\n",
      "\n",
      "  Duplicate data:\n",
      "              IE       UE TARIF  DAYA TGL TEMUAN\n",
      "AAAAAwICAgAFBwEE AAAAAwI=    B1  1300 2024-05-22\n",
      "AAAAAwICAgAFBwEE AAAAAwI=    B1  1300 2025-09-03\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[NORMAL] Duplicates (IE only):\n",
      "  Total duplicate rows: 1345\n",
      "\n",
      "  Duplicate data (showing first 20):\n",
      "              IE TARIF  DAYA\n",
      "AAAAAAACAgACAQQG    R1  2200\n",
      "AAAAAAACAgACAQQG    R1  2200\n",
      "AAAAAAACAgACAwoL    R1   450\n",
      "AAAAAAACAgACAwoL    R1   450\n",
      "AAAAAAACAgEACgIA    R1   450\n",
      "AAAAAAACAgEACgIA    R1   450\n",
      "AAAAAAACAgICBwME    R1   450\n",
      "AAAAAAACAgICBwME    R1   450\n",
      "AAAAAAACAgIDAQQH    B1  1300\n",
      "AAAAAAACAgIDAQQH    B1  1300\n",
      "AAAAAAACAgIEBwID    R1   450\n",
      "AAAAAAACAgIEBwID    R1   450\n",
      "AAAAAAACAgILAAYC    R1   450\n",
      "AAAAAAACAgILAAYC    R1   450\n",
      "AAAAAAACAgMBAAYC   R1M   900\n",
      "AAAAAAACAgMBAAYC   R1M   900\n",
      "AAAAAAACAgMCAgMC    B2 13200\n",
      "AAAAAAACAgMCAgMC    R1   450\n",
      "AAAAAAACAgMCAgMC    B1  1300\n",
      "AAAAAAACAgMCAgMC   R1M   900\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[HISTORY] Duplicates (IE+UE):\n",
      "  Total duplicate rows: 26\n",
      "\n",
      "  Duplicate data (showing first 20):\n",
      "              IE       UE TARIF  DAYA\n",
      "AAAAAAMDAQsAAwAK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsAAwAK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsBAQsK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsBAQsK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsBBAcC AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsBBAcC AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsDBAIA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsDBAIA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsEAgoL AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsEAgoL AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsEAwMB AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsEAwMB AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGAAYA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGAAYA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGAwML AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGAwML AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGBAUK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsGBAUK AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsHBgIA AAAAAAM=   NaN   NaN\n",
      "AAAAAAMDAQsHBgIA AAAAAAM=   NaN   NaN\n",
      "\n",
      "======================================================================\n",
      "OVERLAP CHECK: Fraud vs Normal (IE level)\n",
      "  IE in both Fraud & Normal: 10\n",
      "  Note: FRAUD label wins for overlapping IE\n",
      "  Overlapping IE samples: ['AAAAAgICBQUHBQoA', 'AAAAAgICBgYFAwUL', 'AAAAAgMCBQQFCgEE', 'AAAAAgMCAAMACgQD', 'AAAAAgMCBQICAAEK']\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [3] CHECK DUPLICATES\n",
    "# ============================================================================\n",
    "# For Temuan & History: Check IE+UE (same customer + same unit)\n",
    "# For Normal: Only check IE (no UE column)\n",
    "# Note: Same IE but different UE is NOT a duplicate (same customer, different unit)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DUPLICATE CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TEMUAN: Check IE+UE combination\n",
    "dup_temuan = df_temuan[df_temuan.duplicated(subset=['IE', 'UE'], keep=False)]\n",
    "print(\"\\n[TEMUAN] Duplicates (IE+UE):\")\n",
    "print(f\"  Total duplicate rows: {len(dup_temuan)}\")\n",
    "if len(dup_temuan) > 0:\n",
    "    print(f\"\\n  Duplicate data:\")\n",
    "    print(dup_temuan[['IE', 'UE', 'TARIF', 'DAYA', 'TGL TEMUAN']].sort_values(['IE', 'UE']).to_string(index=False))\n",
    "else:\n",
    "    print(\"  âœ“ No duplicates\")\n",
    "\n",
    "# NORMAL: Check IE only (no UE column)\n",
    "dup_normal = df_normal[df_normal.duplicated(subset=['IE'], keep=False)]\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"[NORMAL] Duplicates (IE only):\")\n",
    "print(f\"  Total duplicate rows: {len(dup_normal)}\")\n",
    "if len(dup_normal) > 0:\n",
    "    print(f\"\\n  Duplicate data (showing first 20):\")\n",
    "    print(dup_normal[['IE', 'TARIF', 'DAYA']].sort_values(['IE']).head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"  âœ“ No duplicates\")\n",
    "\n",
    "# HISTORY: Check IE+UE combination\n",
    "dup_history = df_history[df_history.duplicated(subset=['IE', 'UE'], keep=False)]\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"[HISTORY] Duplicates (IE+UE):\")\n",
    "print(f\"  Total duplicate rows: {len(dup_history)}\")\n",
    "if len(dup_history) > 0:\n",
    "    print(f\"\\n  Duplicate data (showing first 20):\")\n",
    "    cols_to_show = ['IE', 'UE', 'TARIF', 'DAYA']\n",
    "    print(dup_history[cols_to_show].sort_values(['IE', 'UE']).head(20).to_string(index=False))\n",
    "else:\n",
    "    print(\"  âœ“ No duplicates\")\n",
    "\n",
    "# Check overlap between Fraud and Normal (IE level)\n",
    "ie_fraud = set(df_temuan['IE'])\n",
    "ie_normal = set(df_normal['IE'])\n",
    "overlap = ie_fraud & ie_normal\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"OVERLAP CHECK: Fraud vs Normal (IE level)\")\n",
    "print(f\"  IE in both Fraud & Normal: {len(overlap)}\")\n",
    "if len(overlap) > 0:\n",
    "    print(f\"  Note: FRAUD label wins for overlapping IE\")\n",
    "    print(f\"  Overlapping IE samples: {list(overlap)[:5]}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bdbb8a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning duplicates and filtering unsubscribed customers...\n",
      "\n",
      "[TEMUAN]\n",
      "  Before: 177 rows\n",
      "  After : 176 rows\n",
      "  Removed: 1 duplicates\n",
      "\n",
      "[NORMAL]\n",
      "  Before: 10,647 rows\n",
      "  After : 9,920 rows\n",
      "  Removed: 727 duplicates\n",
      "\n",
      "[HISTORY]\n",
      "  Before: 152,974 rows\n",
      "  After : 152,961 rows\n",
      "  Removed: 13 duplicates\n",
      "\n",
      "[OVERLAP HANDLING]\n",
      "  Normal IE overlapping with Fraud: 10\n",
      "  Final Normal count: 9,910\n",
      "\n",
      "[UNSUBSCRIBED CUSTOMERS]\n",
      "  N/A in last 3 months (unsubscribed): 13,904\n",
      "  Removed: 13,904 unsubscribed customers\n",
      "  Remaining: 139,057 active customers\n",
      "\n",
      "âœ“ Cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [4] CLEAN DUPLICATES & FILTER UNSUBSCRIBED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Cleaning duplicates and filtering unsubscribed customers...\")\n",
    "\n",
    "# TEMUAN: Drop duplicates by IE+UE\n",
    "before_temuan = len(df_temuan)\n",
    "df_temuan_clean = df_temuan.drop_duplicates(subset=['IE', 'UE'], keep='last')\n",
    "print(f\"\\n[TEMUAN]\")\n",
    "print(f\"  Before: {before_temuan:,} rows\")\n",
    "print(f\"  After : {len(df_temuan_clean):,} rows\")\n",
    "print(f\"  Removed: {before_temuan - len(df_temuan_clean):,} duplicates\")\n",
    "\n",
    "# NORMAL: Drop duplicates by IE only\n",
    "before_normal = len(df_normal)\n",
    "df_normal_clean = df_normal.drop_duplicates(subset=['IE'], keep='first')\n",
    "print(f\"\\n[NORMAL]\")\n",
    "print(f\"  Before: {before_normal:,} rows\")\n",
    "print(f\"  After : {len(df_normal_clean):,} rows\")\n",
    "print(f\"  Removed: {before_normal - len(df_normal_clean):,} duplicates\")\n",
    "\n",
    "# HISTORY: Drop duplicates by IE+UE\n",
    "before_history = len(df_history)\n",
    "df_history_clean = df_history.drop_duplicates(subset=['IE', 'UE'], keep='first')\n",
    "print(f\"\\n[HISTORY]\")\n",
    "print(f\"  Before: {before_history:,} rows\")\n",
    "print(f\"  After : {len(df_history_clean):,} rows\")\n",
    "print(f\"  Removed: {before_history - len(df_history_clean):,} duplicates\")\n",
    "\n",
    "# Remove overlap from Normal (FRAUD wins)\n",
    "ie_fraud = set(df_temuan_clean['IE'])\n",
    "before_overlap = len(df_normal_clean)\n",
    "df_normal_clean = df_normal_clean[~df_normal_clean['IE'].isin(ie_fraud)]\n",
    "\n",
    "print(f\"\\n[OVERLAP HANDLING]\")\n",
    "print(f\"  Normal IE overlapping with Fraud: {before_overlap - len(df_normal_clean):,}\")\n",
    "print(f\"  Final Normal count: {len(df_normal_clean):,}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FILTER UNSUBSCRIBED CUSTOMERS (N/A at end = tidak berlangganan lagi)\n",
    "# ============================================================================\n",
    "# Extract date columns first\n",
    "non_date_cols = ['IE', 'UE', 'TARIF', 'DAYA']\n",
    "date_columns_temp = [col for col in df_history_clean.columns if col not in non_date_cols]\n",
    "date_columns_temp = sorted(date_columns_temp)\n",
    "\n",
    "# Check last 3 months - if all NaN, customer unsubscribed\n",
    "last_3_cols = date_columns_temp[-3:]\n",
    "\n",
    "# Need to check BEFORE fillna - use original df_history\n",
    "df_temp = df_history.drop_duplicates(subset=['IE', 'UE'], keep='first')\n",
    "unsubscribed_mask = df_temp[last_3_cols].isna().all(axis=1)\n",
    "unsubscribed_count = unsubscribed_mask.sum()\n",
    "\n",
    "print(f\"\\n[UNSUBSCRIBED CUSTOMERS]\")\n",
    "print(f\"  N/A in last 3 months (unsubscribed): {unsubscribed_count:,}\")\n",
    "\n",
    "# Filter out unsubscribed from df_history_clean\n",
    "unsubscribed_ie_ue = set(zip(df_temp[unsubscribed_mask]['IE'], df_temp[unsubscribed_mask]['UE']))\n",
    "before_unsub = len(df_history_clean)\n",
    "\n",
    "# Create mask for filtering (IE, UE) combination\n",
    "mask_unsub = df_history_clean.apply(lambda row: (row['IE'], row['UE']) not in unsubscribed_ie_ue, axis=1)\n",
    "df_history_clean = df_history_clean[mask_unsub]\n",
    "\n",
    "print(f\"  Removed: {before_unsub - len(df_history_clean):,} unsubscribed customers\")\n",
    "print(f\"  Remaining: {len(df_history_clean):,} active customers\")\n",
    "\n",
    "print(\"\\nâœ“ Cleaning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b95631b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels assigned:\n",
      "  Fraud (1)   : 168\n",
      "  Normal (0)  : 9,811\n",
      "  Unknown (-1): 129,078\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [5] LABELING\n",
    "# ============================================================================\n",
    "# Label: 1 = Fraud (Temuan), 0 = Normal, -1 = Unknown\n",
    "ie_fraud = set(df_temuan_clean['IE'])\n",
    "ie_normal = set(df_normal_clean['IE'])\n",
    "\n",
    "def assign_label(ie):\n",
    "    if ie in ie_fraud:\n",
    "        return 1\n",
    "    elif ie in ie_normal:\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "df_history_clean['label'] = df_history_clean['IE'].apply(assign_label)\n",
    "\n",
    "label_counts = df_history_clean['label'].value_counts()\n",
    "print(\"Labels assigned:\")\n",
    "print(f\"  Fraud (1)   : {label_counts.get(1, 0):,}\")\n",
    "print(f\"  Normal (0)  : {label_counts.get(0, 0):,}\")\n",
    "print(f\"  Unknown (-1): {label_counts.get(-1, 0):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5e17ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraud Verification:\n",
      "  Total unique fraud (Temuan)    : 176\n",
      "  Found in History               : 168\n",
      "  NOT in History (missing)       : 8\n",
      "\n",
      "Missing fraud IE samples: ['AAAAAgIHAgEDBQoA', 'AAAAAgIHAgIDBwEG', 'AAAAAgIDAgQCAQUE', 'AAAAAgMCBQYCCgAK', 'AAAAAgMCBgYDBgQL']\n",
      "\n",
      "Actual fraud labeled in dataset  : 168\n",
      "\n",
      "Temuan date range:\n",
      "  First: 2024-01-02 00:00:00\n",
      "  Last : 2025-12-29 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [5B] VERIFY FRAUD DATA - Check missing fraud cases\n",
    "# ============================================================================\n",
    "# Check which fraud IE are NOT in history\n",
    "ie_temuan_all = set(df_temuan_clean['IE'])\n",
    "ie_history_all = set(df_history_clean['IE'])\n",
    "\n",
    "# IE fraud yang tidak ada di history\n",
    "missing_fraud = ie_temuan_all - ie_history_all\n",
    "found_fraud = ie_temuan_all & ie_history_all\n",
    "\n",
    "print(\"Fraud Verification:\")\n",
    "print(f\"  Total unique fraud (Temuan)    : {len(ie_temuan_all):,}\")\n",
    "print(f\"  Found in History               : {len(found_fraud):,}\")\n",
    "print(f\"  NOT in History (missing)       : {len(missing_fraud):,}\")\n",
    "\n",
    "if len(missing_fraud) > 0:\n",
    "    print(f\"\\nMissing fraud IE samples: {list(missing_fraud)[:5]}\")\n",
    "\n",
    "# Check actual labeled fraud in df_history_clean\n",
    "actual_fraud = (df_history_clean['label'] == 1).sum()\n",
    "print(f\"\\nActual fraud labeled in dataset  : {actual_fraud:,}\")\n",
    "\n",
    "# Verify temuan dates - which months have fraud pattern\n",
    "print(\"\\nTemuan date range:\")\n",
    "print(f\"  First: {df_temuan['TGL TEMUAN'].min()}\")\n",
    "print(f\"  Last : {df_temuan['TGL TEMUAN'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e60dc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "  ['UE', 'IE', 'TARIF', 'DAYA', datetime.datetime(2021, 3, 1, 0, 0)] ... (64 total)\n",
      "\n",
      "Data shape: (152948, 64)\n",
      "\n",
      "Data types:\n",
      "float64    60\n",
      "object      3\n",
      "int64       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "TARIF                  13412\n",
      "DAYA                   13412\n",
      "2021-03-01 00:00:00     8944\n",
      "2021-04-01 00:00:00     8881\n",
      "2021-05-01 00:00:00     8826\n",
      "                       ...  \n",
      "2025-09-01 00:00:00    16270\n",
      "2025-10-01 00:00:00    15778\n",
      "2025-11-01 00:00:00    15093\n",
      "2025-12-01 00:00:00    14724\n",
      "2026-01-01 00:00:00    14307\n",
      "Length: 61, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [6] DATA INSPECTION\n",
    "# ============================================================================\n",
    "print(\"Column names:\")\n",
    "print(f\"  {list(df_history_clean.columns[:5])} ... ({len(df_history_clean.columns)} total)\")\n",
    "\n",
    "print(f\"\\nData shape: {df_history_clean.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_history_clean.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "missing = df_history_clean.isnull().sum()\n",
    "missing_cols = missing[missing > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(missing_cols)\n",
    "else:\n",
    "    print(\"  No missing values âœ“\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "93721431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date columns: 59\n",
      "  First: 2021-03-01 00:00:00\n",
      "  Last : 2026-01-01 00:00:00\n",
      "  Range: 59 months\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [7] EXTRACT DATE COLUMNS\n",
    "# ============================================================================\n",
    "# Date columns already extracted during cleaning, just verify\n",
    "print(f\"Date columns: {len(date_columns_temp)}\")\n",
    "print(f\"  First: {date_columns_temp[0]}\")\n",
    "print(f\"  Last : {date_columns_temp[-1]}\")\n",
    "print(f\"  Range: {len(date_columns_temp)} months\")\n",
    "\n",
    "# Use for rest of analysis\n",
    "date_columns = date_columns_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a29bf18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values filled: 2 remaining\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [8] HANDLE MISSING VALUES\n",
    "# ============================================================================\n",
    "# Fill NaN in usage columns with 0\n",
    "df_history_clean[date_columns] = df_history_clean[date_columns].fillna(0)\n",
    "\n",
    "# Check result\n",
    "remaining_na = df_history_clean.isnull().sum().sum()\n",
    "print(f\"Missing values filled: {remaining_na} remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dacb46ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage Statistics:\n",
      "  Min  : 0.00\n",
      "  Max  : 24229392.00\n",
      "  Mean : 595.21\n",
      "  Std  : 20718.42\n",
      "\n",
      "Fraud:\n",
      "  Mean: 218.87\n",
      "  Std : 244.10\n",
      "\n",
      "Normal:\n",
      "  Mean: 304.08\n",
      "  Std : 776.66\n",
      "\n",
      "Unknown:\n",
      "  Mean: 617.83\n",
      "  Std : 21503.14\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [9] BASIC STATISTICS\n",
    "# ============================================================================\n",
    "usage_matrix = df_history_clean[date_columns].values\n",
    "\n",
    "print(\"Usage Statistics:\")\n",
    "print(f\"  Min  : {usage_matrix.min():.2f}\")\n",
    "print(f\"  Max  : {usage_matrix.max():.2f}\")\n",
    "print(f\"  Mean : {usage_matrix.mean():.2f}\")\n",
    "print(f\"  Std  : {usage_matrix.std():.2f}\")\n",
    "\n",
    "# Statistics by label\n",
    "for label in [1, 0, -1]:\n",
    "    label_name = {1: 'Fraud', 0: 'Normal', -1: 'Unknown'}[label]\n",
    "    mask = df_history_clean['label'] == label\n",
    "    if mask.sum() > 0:\n",
    "        values = df_history_clean.loc[mask, date_columns].values\n",
    "        print(f\"\\n{label_name}:\")\n",
    "        print(f\"  Mean: {values.mean():.2f}\")\n",
    "        print(f\"  Std : {values.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "78c338ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TARIF & DAYA ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[TARIF CATEGORIES]\n",
      "Total unique tarif: 18\n",
      "\n",
      "Tarif distribution:\n",
      "  B1             :   6,884 customers ( 4.95%)\n",
      "  B2             :   4,706 customers ( 3.38%)\n",
      "  B3             :     129 customers ( 0.09%)\n",
      "  C              :       4 customers ( 0.00%)\n",
      "  I1             :      26 customers ( 0.02%)\n",
      "  I2             :      99 customers ( 0.07%)\n",
      "  I3             :      38 customers ( 0.03%)\n",
      "  I4             :       1 customers ( 0.00%)\n",
      "  P1             :     940 customers ( 0.68%)\n",
      "  P2             :      33 customers ( 0.02%)\n",
      "  P3             :   2,038 customers ( 1.47%)\n",
      "  R1             :  71,199 customers (51.20%)\n",
      "  R1M            :  44,452 customers (31.97%)\n",
      "  R2             :   3,734 customers ( 2.69%)\n",
      "  R3             :   1,036 customers ( 0.75%)\n",
      "  S1             :   3,717 customers ( 2.67%)\n",
      "  S2             :      15 customers ( 0.01%)\n",
      "  S2K            :       5 customers ( 0.00%)\n",
      "\n",
      "======================================================================\n",
      "DAYA per TARIF\n",
      "======================================================================\n",
      "\n",
      "[B1] - 6,884 customers\n",
      "  Unique DAYA values: 8\n",
      "  DAYA range: 450.0 - 5500.0\n",
      "  Most common DAYA:\n",
      "     450.0 VA:    338 customers ( 4.91%)\n",
      "     900.0 VA:  1,252 customers (18.19%)\n",
      "    1300.0 VA:  1,591 customers (23.11%)\n",
      "    2200.0 VA:  1,934 customers (28.09%)\n",
      "    3500.0 VA:    613 customers ( 8.90%)\n",
      "\n",
      "[B2] - 4,706 customers\n",
      "  Unique DAYA values: 17\n",
      "  DAYA range: 6600.0 - 197000.0\n",
      "  Most common DAYA:\n",
      "    6600.0 VA:    455 customers ( 9.67%)\n",
      "    7700.0 VA:    270 customers ( 5.74%)\n",
      "    10600.0 VA:    966 customers (20.53%)\n",
      "    11000.0 VA:     45 customers ( 0.96%)\n",
      "    13200.0 VA:    586 customers (12.45%)\n",
      "\n",
      "[B3] - 129 customers\n",
      "  Unique DAYA values: 13\n",
      "  DAYA range: 240000.0 - 10350000.0\n",
      "  Most common DAYA:\n",
      "    240000.0 VA:     22 customers (17.05%)\n",
      "    345000.0 VA:     27 customers (20.93%)\n",
      "    555000.0 VA:     22 customers (17.05%)\n",
      "    690000.0 VA:      4 customers ( 3.10%)\n",
      "    865000.0 VA:     12 customers ( 9.30%)\n",
      "\n",
      "[C] - 4 customers\n",
      "  Unique DAYA values: 2\n",
      "  DAYA range: 5500.0 - 23000.0\n",
      "  Most common DAYA:\n",
      "    5500.0 VA:      2 customers (50.00%)\n",
      "    23000.0 VA:      2 customers (50.00%)\n",
      "\n",
      "[I1] - 26 customers\n",
      "  Unique DAYA values: 7\n",
      "  DAYA range: 900.0 - 13200.0\n",
      "  Most common DAYA:\n",
      "     900.0 VA:      5 customers (19.23%)\n",
      "    1300.0 VA:      1 customers ( 3.85%)\n",
      "    2200.0 VA:      5 customers (19.23%)\n",
      "    3500.0 VA:      1 customers ( 3.85%)\n",
      "    6600.0 VA:      3 customers (11.54%)\n",
      "\n",
      "[I2] - 99 customers\n",
      "  Unique DAYA values: 12\n",
      "  DAYA range: 16500.0 - 197000.0\n",
      "  Most common DAYA:\n",
      "    16500.0 VA:      3 customers ( 3.03%)\n",
      "    23000.0 VA:     11 customers (11.11%)\n",
      "    33000.0 VA:     10 customers (10.10%)\n",
      "    41500.0 VA:      3 customers ( 3.03%)\n",
      "    53000.0 VA:     11 customers (11.11%)\n",
      "\n",
      "[I3] - 38 customers\n",
      "  Unique DAYA values: 13\n",
      "  DAYA range: 240000.0 - 13800000.0\n",
      "  Most common DAYA:\n",
      "    240000.0 VA:      1 customers ( 2.63%)\n",
      "    345000.0 VA:      8 customers (21.05%)\n",
      "    555000.0 VA:      7 customers (18.42%)\n",
      "    690000.0 VA:      3 customers ( 7.89%)\n",
      "    865000.0 VA:      3 customers ( 7.89%)\n",
      "\n",
      "[I4] - 1 customers\n",
      "  Unique DAYA values: 1\n",
      "  DAYA range: 140000000.0 - 140000000.0\n",
      "  Most common DAYA:\n",
      "    140000000.0 VA:      1 customers (100.00%)\n",
      "\n",
      "[P1] - 940 customers\n",
      "  Unique DAYA values: 26\n",
      "  DAYA range: 450.0 - 197000.0\n",
      "  Most common DAYA:\n",
      "     450.0 VA:     19 customers ( 2.02%)\n",
      "     900.0 VA:    106 customers (11.28%)\n",
      "    1300.0 VA:     86 customers ( 9.15%)\n",
      "    2200.0 VA:     88 customers ( 9.36%)\n",
      "    3500.0 VA:     48 customers ( 5.11%)\n",
      "\n",
      "[P2] - 33 customers\n",
      "  Unique DAYA values: 7\n",
      "  DAYA range: 240000.0 - 3465000.0\n",
      "  Most common DAYA:\n",
      "    240000.0 VA:      6 customers (18.18%)\n",
      "    345000.0 VA:      6 customers (18.18%)\n",
      "    555000.0 VA:     12 customers (36.36%)\n",
      "    690000.0 VA:      3 customers ( 9.09%)\n",
      "    1110000.0 VA:      2 customers ( 6.06%)\n",
      "\n",
      "[P3] - 2,038 customers\n",
      "  Unique DAYA values: 36\n",
      "  DAYA range: 450.0 - 2683787.0\n",
      "  Most common DAYA:\n",
      "     450.0 VA:      3 customers ( 0.15%)\n",
      "     900.0 VA:     52 customers ( 2.55%)\n",
      "    1035.0 VA:      1 customers ( 0.05%)\n",
      "    1300.0 VA:    176 customers ( 8.64%)\n",
      "    1800.0 VA:      1 customers ( 0.05%)\n",
      "\n",
      "[R1] - 71,199 customers\n",
      "  Unique DAYA values: 4\n",
      "  DAYA range: 450.0 - 2200.0\n",
      "  Most common DAYA:\n",
      "     450.0 VA: 20,611 customers (28.95%)\n",
      "     900.0 VA: 12,455 customers (17.49%)\n",
      "    1300.0 VA: 24,687 customers (34.67%)\n",
      "    2200.0 VA: 13,446 customers (18.89%)\n",
      "\n",
      "[R1M] - 44,452 customers\n",
      "  Unique DAYA values: 1\n",
      "  DAYA range: 900.0 - 900.0\n",
      "  Most common DAYA:\n",
      "     900.0 VA: 44,452 customers (100.00%)\n",
      "\n",
      "[R2] - 3,734 customers\n",
      "  Unique DAYA values: 4\n",
      "  DAYA range: 3500.0 - 5500.0\n",
      "  Most common DAYA:\n",
      "    3500.0 VA:  1,814 customers (48.58%)\n",
      "    3900.0 VA:     33 customers ( 0.88%)\n",
      "    4400.0 VA:    809 customers (21.67%)\n",
      "    5500.0 VA:  1,078 customers (28.87%)\n",
      "\n",
      "[R3] - 1,036 customers\n",
      "  Unique DAYA values: 19\n",
      "  DAYA range: 6600.0 - 1730000.0\n",
      "  Most common DAYA:\n",
      "    6600.0 VA:    183 customers (17.66%)\n",
      "    7700.0 VA:    273 customers (26.35%)\n",
      "    10600.0 VA:    165 customers (15.93%)\n",
      "    11000.0 VA:     30 customers ( 2.90%)\n",
      "    13200.0 VA:    112 customers (10.81%)\n",
      "\n",
      "[S1] - 3,717 customers\n",
      "  Unique DAYA values: 24\n",
      "  DAYA range: 450.0 - 197000.0\n",
      "  Most common DAYA:\n",
      "     450.0 VA:    201 customers ( 5.41%)\n",
      "     900.0 VA:    728 customers (19.59%)\n",
      "    1300.0 VA:    423 customers (11.38%)\n",
      "    2200.0 VA:    431 customers (11.60%)\n",
      "    3500.0 VA:    239 customers ( 6.43%)\n",
      "\n",
      "[S2] - 15 customers\n",
      "  Unique DAYA values: 8\n",
      "  DAYA range: 240000.0 - 2180000.0\n",
      "  Most common DAYA:\n",
      "    240000.0 VA:      1 customers ( 6.67%)\n",
      "    555000.0 VA:      3 customers (20.00%)\n",
      "    690000.0 VA:      1 customers ( 6.67%)\n",
      "    865000.0 VA:      5 customers (33.33%)\n",
      "    1110000.0 VA:      1 customers ( 6.67%)\n",
      "\n",
      "[S2K] - 5 customers\n",
      "  Unique DAYA values: 5\n",
      "  DAYA range: 345000.0 - 1385000.0\n",
      "  Most common DAYA:\n",
      "    345000.0 VA:      1 customers (20.00%)\n",
      "    555000.0 VA:      1 customers (20.00%)\n",
      "    690000.0 VA:      1 customers (20.00%)\n",
      "    1110000.0 VA:      1 customers (20.00%)\n",
      "    1385000.0 VA:      1 customers (20.00%)\n",
      "\n",
      "[nan] - 1 customers\n",
      "  Unique DAYA values: 0\n",
      "  DAYA range: nan - nan\n",
      "  Most common DAYA:\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Total TARIF categories: 18\n",
      "Total unique DAYA values: 62\n",
      "DAYA range (overall): 450.0 - 140000000.0 VA\n",
      "\n",
      "[BREAKDOWN BY LABEL]\n",
      "\n",
      "Fraud (168 customers):\n",
      "  Tarif categories: 7\n",
      "  Top 3 tarif: R1, R1M, B1\n",
      "  DAYA range: 450.0 - 5500.0 VA\n",
      "\n",
      "Normal (9,811 customers):\n",
      "  Tarif categories: 11\n",
      "  Top 3 tarif: R1, R1M, B1\n",
      "  DAYA range: 450.0 - 197000.0 VA\n",
      "\n",
      "Unknown (129,078 customers):\n",
      "  Tarif categories: 18\n",
      "  Top 3 tarif: R1, R1M, B1\n",
      "  DAYA range: 450.0 - 140000000.0 VA\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [11] TARIF & DAYA ANALYSIS (for Visualization)\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"TARIF & DAYA ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TARIF Categories\n",
    "print(\"\\n[TARIF CATEGORIES]\")\n",
    "tarif_counts = df_history_clean['TARIF'].value_counts().sort_index()\n",
    "print(f\"Total unique tarif: {len(tarif_counts)}\")\n",
    "print(f\"\\nTarif distribution:\")\n",
    "for tarif, count in tarif_counts.items():\n",
    "    pct = (count / len(df_history_clean)) * 100\n",
    "    print(f\"  {str(tarif):15s}: {count:>7,} customers ({pct:>5.2f}%)\")\n",
    "\n",
    "# DAYA per TARIF\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DAYA per TARIF\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Sort tarif by converting to string first\n",
    "tarif_list = df_history_clean['TARIF'].unique()\n",
    "tarif_list_str = sorted([str(t) for t in tarif_list])\n",
    "\n",
    "for tarif_str in tarif_list_str:\n",
    "    # Find original tarif value (could be string or number)\n",
    "    df_tarif = df_history_clean[df_history_clean['TARIF'].astype(str) == tarif_str]\n",
    "    daya_values = df_tarif['DAYA'].value_counts().sort_index()\n",
    "    \n",
    "    print(f\"\\n[{tarif_str}] - {len(df_tarif):,} customers\")\n",
    "    print(f\"  Unique DAYA values: {len(daya_values)}\")\n",
    "    print(f\"  DAYA range: {df_tarif['DAYA'].min()} - {df_tarif['DAYA'].max()}\")\n",
    "    print(f\"  Most common DAYA:\")\n",
    "    \n",
    "    # Show top 5 most common DAYA values\n",
    "    for daya, count in daya_values.head(5).items():\n",
    "        pct = (count / len(df_tarif)) * 100\n",
    "        print(f\"    {daya:>6} VA: {count:>6,} customers ({pct:>5.2f}%)\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total TARIF categories: {df_history_clean['TARIF'].nunique()}\")\n",
    "print(f\"Total unique DAYA values: {df_history_clean['DAYA'].nunique()}\")\n",
    "print(f\"DAYA range (overall): {df_history_clean['DAYA'].min()} - {df_history_clean['DAYA'].max()} VA\")\n",
    "\n",
    "# Breakdown by label\n",
    "print(\"\\n[BREAKDOWN BY LABEL]\")\n",
    "for label_val in [1, 0, -1]:\n",
    "    label_name = {1: 'Fraud', 0: 'Normal', -1: 'Unknown'}[label_val]\n",
    "    df_label = df_history_clean[df_history_clean['label'] == label_val]\n",
    "    if len(df_label) > 0:\n",
    "        print(f\"\\n{label_name} ({len(df_label):,} customers):\")\n",
    "        print(f\"  Tarif categories: {df_label['TARIF'].nunique()}\")\n",
    "        print(f\"  Top 3 tarif: {', '.join([str(t) for t in df_label['TARIF'].value_counts().head(3).index.tolist()])}\")\n",
    "        print(f\"  DAYA range: {df_label['DAYA'].min()} - {df_label['DAYA'].max()} VA\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b1140",
   "metadata": {},
   "source": [
    "# ðŸ”¬ DATA PREPARATION FOR MACHINE LEARNING\n",
    "---\n",
    "**Dataset Split:**\n",
    "- **Training:** Fraud + Normal (label = 1, 0)\n",
    "- **Testing:** Unknown (label = -1) â†’ untuk prediksi nanti\n",
    "\n",
    "**Data Preparation Steps:**\n",
    "1. âœ… Split Data (Train vs Test)\n",
    "2. âœ… Feature Extraction (21 features)\n",
    "3. âœ… Normalization/Scaling (StandardScaler)\n",
    "\n",
    "**Note:** Data sudah siap untuk modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f7c7b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DATA SPLITTING\n",
      "======================================================================\n",
      "\n",
      "[TRAINING SET] Fraud + Normal\n",
      "  Total: 9,979 customers\n",
      "  Fraud  (1): 168\n",
      "  Normal (0): 9,811\n",
      "\n",
      "[TEST SET] Unknown\n",
      "  Total: 129,078 customers\n",
      "  Will be predicted later\n",
      "\n",
      "[CLASS IMBALANCE]\n",
      "  Normal : Fraud = 9,811 : 168\n",
      "  Imbalance ratio: 1:58.4\n",
      "  âš ï¸  Highly imbalanced â†’ Will use SMOTE later\n",
      "\n",
      "âœ“ Data split complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [12] SPLIT DATA: Train (Fraud+Normal) vs Test (Unknown)\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training data: Fraud (1) + Normal (0)\n",
    "df_train = df_history_clean[df_history_clean['label'].isin([0, 1])].copy()\n",
    "\n",
    "# Test data: Unknown (-1) - akan diprediksi nanti\n",
    "df_test = df_history_clean[df_history_clean['label'] == -1].copy()\n",
    "\n",
    "print(f\"\\n[TRAINING SET] Fraud + Normal\")\n",
    "print(f\"  Total: {len(df_train):,} customers\")\n",
    "print(f\"  Fraud  (1): {(df_train['label'] == 1).sum():,}\")\n",
    "print(f\"  Normal (0): {(df_train['label'] == 0).sum():,}\")\n",
    "\n",
    "print(f\"\\n[TEST SET] Unknown\")\n",
    "print(f\"  Total: {len(df_test):,} customers\")\n",
    "print(f\"  Will be predicted later\")\n",
    "\n",
    "# Class imbalance check\n",
    "fraud_count = (df_train['label'] == 1).sum()\n",
    "normal_count = (df_train['label'] == 0).sum()\n",
    "imbalance_ratio = normal_count / fraud_count\n",
    "\n",
    "print(f\"\\n[CLASS IMBALANCE]\")\n",
    "print(f\"  Normal : Fraud = {normal_count:,} : {fraud_count:,}\")\n",
    "print(f\"  Imbalance ratio: 1:{imbalance_ratio:.1f}\")\n",
    "print(f\"  âš ï¸  Highly imbalanced â†’ Will use SMOTE later\")\n",
    "\n",
    "print(\"\\nâœ“ Data split complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6924dc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "[EXTRACTING FEATURES - TRAINING SET]\n",
      "  Training features shape: (9979, 18)\n",
      "  Training labels shape: (9979,)\n",
      "\n",
      "[EXTRACTING FEATURES - TEST SET]\n",
      "  Test features shape: (129078, 18)\n",
      "\n",
      "[FEATURE LIST] 18 features:\n",
      "   1. mean_usage\n",
      "   2. std_usage\n",
      "   3. cv_usage\n",
      "   4. max_usage\n",
      "   5. min_usage\n",
      "   6. range_usage\n",
      "   7. first_half_mean\n",
      "   8. last_half_mean\n",
      "   9. half_diff_pct\n",
      "  10. year_change_pct\n",
      "  11. max_drop\n",
      "  12. n_large_drops\n",
      "  13. n_spikes\n",
      "  14. zero_count\n",
      "  15. zero_ratio\n",
      "  16. max_zero_run\n",
      "  17. recent_3m_mean\n",
      "  18. recent_vs_overall\n",
      "\n",
      "âœ“ Feature extraction complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [13] FEATURE EXTRACTION\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "class ElectricityTheftFeatureExtractor:\n",
    "    \"\"\"Extract features from electricity usage patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, date_columns):\n",
    "        self.date_columns = date_columns\n",
    "        self.n_months = len(date_columns)\n",
    "        \n",
    "    def extract_all_features(self, df):\n",
    "        \"\"\"Extract all features\"\"\"\n",
    "        features_data = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            usage = row[self.date_columns].values\n",
    "            \n",
    "            # Extract features\n",
    "            features = {}\n",
    "            \n",
    "            # === COMPARISON FEATURES ===\n",
    "            features['mean_usage'] = np.mean(usage)\n",
    "            features['std_usage'] = np.std(usage)\n",
    "            features['cv_usage'] = features['std_usage'] / (features['mean_usage'] + 1e-6)\n",
    "            features['max_usage'] = np.max(usage)\n",
    "            features['min_usage'] = np.min(usage)\n",
    "            features['range_usage'] = features['max_usage'] - features['min_usage']\n",
    "            \n",
    "            # === PATTERN DETECTION ===\n",
    "            # First vs Last half comparison\n",
    "            mid = len(usage) // 2\n",
    "            first_half = usage[:mid]\n",
    "            last_half = usage[mid:]\n",
    "            features['first_half_mean'] = np.mean(first_half)\n",
    "            features['last_half_mean'] = np.mean(last_half)\n",
    "            features['half_diff_pct'] = ((features['last_half_mean'] - features['first_half_mean']) / \n",
    "                                         (features['first_half_mean'] + 1e-6)) * 100\n",
    "            \n",
    "            # Year-over-year comparison (if enough data)\n",
    "            if len(usage) >= 24:\n",
    "                first_year = usage[:12]\n",
    "                last_year = usage[-12:]\n",
    "                features['year_change_pct'] = ((np.mean(last_year) - np.mean(first_year)) / \n",
    "                                               (np.mean(first_year) + 1e-6)) * 100\n",
    "            else:\n",
    "                features['year_change_pct'] = 0\n",
    "            \n",
    "            # === LOSS & PRIORITY FEATURES ===\n",
    "            # Sudden drops\n",
    "            diff = np.diff(usage)\n",
    "            features['max_drop'] = np.min(diff) if len(diff) > 0 else 0\n",
    "            features['n_large_drops'] = np.sum(diff < -100)\n",
    "            \n",
    "            # Spikes\n",
    "            features['n_spikes'] = np.sum(diff > 200)\n",
    "            \n",
    "            # === ZERO PATTERN ===\n",
    "            features['zero_count'] = np.sum(usage == 0)\n",
    "            features['zero_ratio'] = features['zero_count'] / len(usage)\n",
    "            \n",
    "            # Zero runs (consecutive zeros)\n",
    "            zero_runs = []\n",
    "            current_run = 0\n",
    "            for val in usage:\n",
    "                if val == 0:\n",
    "                    current_run += 1\n",
    "                else:\n",
    "                    if current_run > 0:\n",
    "                        zero_runs.append(current_run)\n",
    "                    current_run = 0\n",
    "            if current_run > 0:\n",
    "                zero_runs.append(current_run)\n",
    "            features['max_zero_run'] = max(zero_runs) if zero_runs else 0\n",
    "            \n",
    "            # === TEMPORAL FEATURES ===\n",
    "            # Recent 3 months vs overall\n",
    "            recent_3months = usage[-3:]\n",
    "            features['recent_3m_mean'] = np.mean(recent_3months)\n",
    "            features['recent_vs_overall'] = ((features['recent_3m_mean'] - features['mean_usage']) / \n",
    "                                            (features['mean_usage'] + 1e-6)) * 100\n",
    "            \n",
    "            features_data.append(features)\n",
    "        \n",
    "        return pd.DataFrame(features_data)\n",
    "\n",
    "# Extract features for training data\n",
    "print(\"\\n[EXTRACTING FEATURES - TRAINING SET]\")\n",
    "extractor = ElectricityTheftFeatureExtractor(date_columns)\n",
    "X_train_features = extractor.extract_all_features(df_train)\n",
    "y_train = df_train['label'].values\n",
    "\n",
    "print(f\"  Training features shape: {X_train_features.shape}\")\n",
    "print(f\"  Training labels shape: {y_train.shape}\")\n",
    "\n",
    "# Extract features for test data  \n",
    "print(\"\\n[EXTRACTING FEATURES - TEST SET]\")\n",
    "X_test_features = extractor.extract_all_features(df_test)\n",
    "\n",
    "print(f\"  Test features shape: {X_test_features.shape}\")\n",
    "\n",
    "print(f\"\\n[FEATURE LIST] {X_train_features.shape[1]} features:\")\n",
    "for i, col in enumerate(X_train_features.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(\"\\nâœ“ Feature extraction complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fcf9c155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FEATURE SCALING (StandardScaler)\n",
      "======================================================================\n",
      "\n",
      "[TRAINING SET]\n",
      "  Original shape: (9979, 18)\n",
      "  Scaled shape: (9979, 18)\n",
      "  Mean after scaling: 0.000000\n",
      "  Std after scaling: 1.000000\n",
      "\n",
      "[TEST SET]\n",
      "  Original shape: (129078, 18)\n",
      "  Scaled shape: (129078, 18)\n",
      "\n",
      "âœ“ Feature scaling complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [14] NORMALIZATION / SCALING\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE SCALING (StandardScaler)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train_features)\n",
    "print(f\"\\n[TRAINING SET]\")\n",
    "print(f\"  Original shape: {X_train_features.shape}\")\n",
    "print(f\"  Scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Mean after scaling: {X_train_scaled.mean():.6f}\")\n",
    "print(f\"  Std after scaling: {X_train_scaled.std():.6f}\")\n",
    "\n",
    "# Transform test data (using same scaler)\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "print(f\"\\n[TEST SET]\")\n",
    "print(f\"  Original shape: {X_test_features.shape}\")\n",
    "print(f\"  Scaled shape: {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"\\nâœ“ Feature scaling complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "50731efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ“Š DATA PREPARATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "[1] RAW DATA\n",
      "  Total customers: 139,057\n",
      "  Time period: 59 months (Mar 2021 - Jan 2026)\n",
      "  - Fraud (known): 168\n",
      "  - Normal (known): 9,811\n",
      "  - Unknown: 129,078\n",
      "\n",
      "[2] DATA SPLIT\n",
      "  Training Set: 9,979 customers\n",
      "    - Fraud  (1): 168 (1.68%)\n",
      "    - Normal (0): 9,811 (98.32%)\n",
      "    - Imbalance ratio: 1:58.4\n",
      "  Test Set: 129,078 customers (Unknown)\n",
      "\n",
      "[3] FEATURE EXTRACTION\n",
      "  Features extracted: 18\n",
      "  Training features: (9979, 18)\n",
      "  Test features: (129078, 18)\n",
      "\n",
      "  Feature categories:\n",
      "    â€¢ Comparison: mean, std, CV, max, min, range\n",
      "    â€¢ Pattern Detection: first/last half comparison, year-over-year\n",
      "    â€¢ Loss & Priority: sudden drops, spikes\n",
      "    â€¢ Zero Pattern: zero count, ratio, max consecutive zeros\n",
      "    â€¢ Temporal: recent 3 months vs overall\n",
      "\n",
      "[4] NORMALIZATION\n",
      "  Method: StandardScaler (mean=0, std=1)\n",
      "  Training scaled: (9979, 18)\n",
      "  Test scaled: (129078, 18)\n",
      "\n",
      "======================================================================\n",
      "âœ… DATA PREPARATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ðŸ’¡ Next Steps:\n",
      "  1. Handle class imbalance (e.g., SMOTE)\n",
      "  2. Train classification models\n",
      "  3. Evaluate model performance\n",
      "  4. Predict Unknown customers\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# [16] DATA PREPARATION SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ“Š DATA PREPARATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n[1] RAW DATA\")\n",
    "print(f\"  Total customers: {len(df_history_clean):,}\")\n",
    "print(f\"  Time period: {len(date_columns)} months (Mar 2021 - Jan 2026)\")\n",
    "print(f\"  - Fraud (known): {(df_history_clean['label'] == 1).sum():,}\")\n",
    "print(f\"  - Normal (known): {(df_history_clean['label'] == 0).sum():,}\")\n",
    "print(f\"  - Unknown: {(df_history_clean['label'] == -1).sum():,}\")\n",
    "\n",
    "print(\"\\n[2] DATA SPLIT\")\n",
    "print(f\"  Training Set: {len(df_train):,} customers\")\n",
    "print(f\"    - Fraud  (1): {(y_train == 1).sum():,} ({(y_train == 1).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"    - Normal (0): {(y_train == 0).sum():,} ({(y_train == 0).sum()/len(y_train)*100:.2f}%)\")\n",
    "print(f\"    - Imbalance ratio: 1:{(y_train == 0).sum() / (y_train == 1).sum():.1f}\")\n",
    "print(f\"  Test Set: {len(df_test):,} customers (Unknown)\")\n",
    "\n",
    "print(\"\\n[3] FEATURE EXTRACTION\")\n",
    "print(f\"  Features extracted: {X_train_features.shape[1]}\")\n",
    "print(f\"  Training features: {X_train_features.shape}\")\n",
    "print(f\"  Test features: {X_test_features.shape}\")\n",
    "print(f\"\\n  Feature categories:\")\n",
    "print(f\"    â€¢ Comparison: mean, std, CV, max, min, range\")\n",
    "print(f\"    â€¢ Pattern Detection: first/last half comparison, year-over-year\")\n",
    "print(f\"    â€¢ Loss & Priority: sudden drops, spikes\")\n",
    "print(f\"    â€¢ Zero Pattern: zero count, ratio, max consecutive zeros\")\n",
    "print(f\"    â€¢ Temporal: recent 3 months vs overall\")\n",
    "\n",
    "print(\"\\n[4] NORMALIZATION\")\n",
    "print(f\"  Method: StandardScaler (mean=0, std=1)\")\n",
    "print(f\"  Training scaled: {X_train_scaled.shape}\")\n",
    "print(f\"  Test scaled: {X_test_scaled.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… DATA PREPARATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nðŸ’¡ Next Steps:\")\n",
    "print(\"  1. Handle class imbalance (e.g., SMOTE)\")\n",
    "print(\"  2. Train classification models\")\n",
    "print(\"  3. Evaluate model performance\")\n",
    "print(\"  4. Predict Unknown customers\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a3e05",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
